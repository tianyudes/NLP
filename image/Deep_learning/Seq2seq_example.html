<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8" /><title>DeepLearning における会話モデル： Seq2Seq から VHRED まで #DeepLearning - Qiita</title><meta content="こんにちは。 DeepLearning で対話ロボットを作ろうとしているインコです。この記事は mixi Advent Calendar 2017 の 12/03 の記事です。概要近年対話モデ…" name="description" /><meta content="width=device-width,initial-scale=1,shrink-to-fit=no,viewport-fit=cover" name="viewport" /><meta content="#ffffff" name="theme-color" /><meta content="XWpkTG32-_C4joZoJ_UsmDUi-zaH-hcrjF6ZC_FoFbk" name="google-site-verification" /><meta content="telephone=no" name="format-detection" /><link rel="canonical" href="https://qiita.com/halhorn/items/646d323ac457715866d4" /><link href="/manifest.json" rel="manifest" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link as="script" href="https://www.googletagservices.com/tag/js/gpt.js" rel="preload" /><link href="https://securepubads.g.doubleclick.net" rel="preconnect" /><script async="" src="https://www.googletagservices.com/tag/js/gpt.js"></script><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="kxE5fKX0oLjT79arxyepf4fhSQ_st4K2MxmR51EfkMI1HUQ8TWGv_EigjFsBc25XuGmZETx8Bes_Upmv2P7D_A" /><link rel="icon" type="image/x-icon" href="https://cdn.qiita.com/assets/favicons/public/production-c620d3e403342b1022967ba5e3db1aaa.ico" /><link rel="apple-touch-icon" type="image/png" href="https://cdn.qiita.com/assets/favicons/public/apple-touch-icon-ec5ba42a24ae923f16825592efdc356f.png" /><link rel="stylesheet" href="https://cdn.qiita.com/assets/public/article-7baeae19fe75e4cdebfd0525721905fe.min.css" media="all" /><link rel="stylesheet" href="https://cdn.qiita.com/assets/public/dark-b8a4a672eb82ffd8775460b87c7bc28c.min.css" media="all" /><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,500,0..1,0" media="all" /><script src="https://cdn.qiita.com/assets/public/v3-article-bundle-3e6c59d1956d8f14.min.js" defer="defer"></script><meta name="twitter:card" content="summary_large_image"><meta content="@Qiita" name="twitter:site" /><meta content="@halhorn" name="twitter:creator" /><meta property="og:type" content="article"><meta property="og:title" content="DeepLearning における会話モデル： Seq2Seq から VHRED まで - Qiita"><meta property="og:image" content="https://qiita-user-contents.imgix.net/https%3A%2F%2Fcdn.qiita.com%2Fassets%2Fpublic%2Fadvent-calendar-ogp-background-f625e957b80c4bd8dd47b724be996090.jpg?ixlib=rb-4.0.0&amp;w=1200&amp;mark64=aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTkxNiZoPTMzNiZ0eHQ9RGVlcExlYXJuaW5nJTIwJUUzJTgxJUFCJUUzJTgxJThBJUUzJTgxJTkxJUUzJTgyJThCJUU0JUJDJTlBJUU4JUE5JUIxJUUzJTgzJUEyJUUzJTgzJTg3JUUzJTgzJUFCJUVGJUJDJTlBJTIwU2VxMlNlcSUyMCVFMyU4MSU4QiVFMyU4MiU4OSUyMFZIUkVEJTIwJUUzJTgxJUJFJUUzJTgxJUE3JnR4dC1jb2xvcj0lMjMzQTNDM0MmdHh0LWZvbnQ9SGlyYWdpbm8lMjBTYW5zJTIwVzYmdHh0LXNpemU9NTYmdHh0LWNsaXA9ZWxsaXBzaXMmdHh0LWFsaWduPWxlZnQlMkNtaWRkbGUmcz1jZDNjZTE2NTk1ZDkxYzU2NGU3MDlmZGUyMDVlYzViMw&amp;mark-x=142&amp;mark-y=151&amp;blend64=aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTcxNiZ0eHQ9JTQwaGFsaG9ybiUyMGluJTIwJUU2JUEwJUFBJUU1JUJDJThGJUU0JUJDJTlBJUU3JUE0JUJFTUlYSSZ0eHQtY29sb3I9JTIzM0EzQzNDJnR4dC1mb250PUhpcmFnaW5vJTIwU2FucyUyMFc2JnR4dC1zaXplPTMyJnR4dC1hbGlnbj1sZWZ0JTJDdG9wJnM9ODZiNjY1MzA0NDY0YTIxZDUwNTI3YmQ1MWE5YmIxOGU&amp;blend-x=142&amp;blend-y=491&amp;blend-mode=normal&amp;s=9fcf37782e42a812b75dbba705687de2"><meta property="og:description" content="こんにちは。 DeepLearning で対話ロボットを作ろうとしているインコです。この記事は mixi Advent Calendar 2017 の 12/03 の記事です。概要近年対話モデ…"><meta content="https://qiita.com/halhorn/items/646d323ac457715866d4" property="og:url" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><meta content="NLP,DeepLearning,VHRED" name="keywords" /><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?
n.callMethod.apply(n,arguments):n.queue.push(arguments)};
if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
n.queue=[];t=b.createElement(e);t.async=!0;
t.src=v;s=b.getElementsByTagName(e)[0];
s.parentNode.insertBefore(t,s)}(window, document,'script',
'https://connect.facebook.net/en_US/fbevents.js');
fbq('init', '305156090176370');
fbq('trackSingle', '305156090176370', 'PageView');</script><noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=305156090176370&ev=PageView&noscript=1"/></noscript><style data-emotion="style-global 92283">.style-92283{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="style-global xgff9b">.style-xgff9b{height:32px;}</style><style data-emotion="style-global qgd36e">.style-qgd36e{background:var(--color-background);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:56px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:16px 24px 0;}@media (max-width: 1199px){.style-qgd36e{gap:24px;}}@media (max-width: 769px){.style-qgd36e{padding:16px 16px 0;}}@media (max-width: 479px){.style-qgd36e{padding:16px 0 0;}}</style><style data-emotion="style-global yrmhnf">.style-yrmhnf{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:16px;height:-webkit-fit-content;height:-moz-fit-content;height:fit-content;position:-webkit-sticky;position:sticky;top:16px;z-index:5;}@media (max-width: 769px){.style-yrmhnf{display:none;}}</style><style data-emotion="style-global 2pgeei">.style-2pgeei{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="style-global 1duml6f">.style-1duml6f{height:40px;width:40px;position:relative;}.style-1duml6f svg{cursor:auto;pointer-events:none;}</style><style data-emotion="style-global zq6eqh">.style-zq6eqh{fill:none;left:50%;position:absolute;top:50%;-webkit-transform:translate(-50%, -50%);-moz-transform:translate(-50%, -50%);-ms-transform:translate(-50%, -50%);transform:translate(-50%, -50%);}.style-zq6eqh .circle,.style-zq6eqh .heart-stroke,.style-zq6eqh .heart-fill,.style-zq6eqh .particle{-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-timing-function:ease-in-out;animation-timing-function:ease-in-out;transform-origin:center center;}.style-zq6eqh .circle{fill:var(--color-surface);stroke:var(--color-divider);stroke-width:1;}.style-zq6eqh .heart-stroke{fill:var(--color-mediumEmphasis);}.style-zq6eqh .heart-fill{fill:var(--color-green60);opacity:0;}.style-zq6eqh .particles{rotate:var(--rotate);transform-origin:center center;}.style-zq6eqh .particle{opacity:0;}.style-zq6eqh.liked .circle{-webkit-animation-delay:90ms;animation-delay:90ms;-webkit-animation-duration:60ms;animation-duration:60ms;-webkit-animation-name:animation-1mcqpj6;animation-name:animation-1mcqpj6;}.style-zq6eqh.liked .heart-stroke{-webkit-animation-duration:150ms;animation-duration:150ms;-webkit-animation-name:animation-1gt78ug;animation-name:animation-1gt78ug;}.style-zq6eqh.liked .heart-fill{-webkit-animation-delay:150ms;animation-delay:150ms;-webkit-animation-duration:100ms;animation-duration:100ms;-webkit-animation-name:animation-19addpb;animation-name:animation-19addpb;}.style-zq6eqh.liked .particle{-webkit-animation-delay:150ms;animation-delay:150ms;-webkit-animation-duration:var(--duration);animation-duration:var(--duration);-webkit-animation-name:animation-1spcwjj;animation-name:animation-1spcwjj;}</style><style data-emotion="style-global animation-1spcwjj">@-webkit-keyframes animation-1spcwjj{1%{opacity:1;}90%{opacity:1;}100%{cx:var(--x);cy:var(--y);opacity:0;}}@keyframes animation-1spcwjj{1%{opacity:1;}90%{opacity:1;}100%{cx:var(--x);cy:var(--y);opacity:0;}}</style><style data-emotion="style-global animation-19addpb">@-webkit-keyframes animation-19addpb{1%{scale:1.2;opacity:1;}100%{scale:1;opacity:1;}}@keyframes animation-19addpb{1%{scale:1.2;opacity:1;}100%{scale:1;opacity:1;}}</style><style data-emotion="style-global animation-1gt78ug">@-webkit-keyframes animation-1gt78ug{40%{fill:currentColor;scale:0.85px;}100%{fill:var(--color-green60);}}@keyframes animation-1gt78ug{40%{fill:currentColor;scale:0.85px;}100%{fill:var(--color-green60);}}</style><style data-emotion="style-global animation-1mcqpj6">@-webkit-keyframes animation-1mcqpj6{100%{stroke:var(--color-green60);}}@keyframes animation-1mcqpj6{100%{stroke:var(--color-green60);}}</style><style data-emotion="style-global 10ttvi6">.style-10ttvi6{background-color:transparent;border:none;cursor:pointer;height:40px;padding:0;position:absolute;width:40px;}.style-10ttvi6:disabled{cursor:not-allowed;}</style><style data-emotion="style-global klat6i">.style-klat6i{color:var(--color-mediumEmphasis);cursor:pointer;font-size:var(--font-size-body-2);font-weight:bold;}</style><style data-emotion="style-global krcte5">.style-krcte5{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="style-global 1xwfn2v">.style-1xwfn2v{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;background-color:var(--color-surface);border:1px solid var(--color-divider);border-radius:50%;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:40px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:40px;}</style><style data-emotion="style-global 100tu0r">.style-100tu0r{display:inline-block;fill:var(--color-mediumEmphasis);height:24px;width:24px;}</style><style data-emotion="style-global 5ji4gp">.style-5ji4gp{color:var(--color-mediumEmphasis);font-size:14px;font-weight:bold;}</style><style data-emotion="style-global 1n9ulpr">.style-1n9ulpr{border:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;background-color:transparent;border:none;color:var(--color-mediumEmphasis);cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:20px;height:32px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:0;width:32px;}</style><style data-emotion="style-global 1b1cd5z">.style-1b1cd5z{height:20px;fill:var(--color-twitter);width:20px;}</style><style data-emotion="style-global 157xj1g">.style-157xj1g{height:20px;fill:#1877f2;width:20px;}</style><style data-emotion="style-global 1uu7u3w">.style-1uu7u3w{height:20px;width:20px;}</style><style data-emotion="style-global 79elbk">.style-79elbk{position:relative;}</style><style data-emotion="style-global rvwogb">.style-rvwogb{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;background-color:transparent;border:none;border-radius:50%;color:var(--color-mediumEmphasis);cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:var(--font-size-subhead-1);height:32px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:0;width:32px;}</style><style data-emotion="style-global 1j8uoc8">.style-1j8uoc8{display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:24px;font-variation-settings:'FILL' 0,'wght' 500,'GRAD' 0,'opsz' 24;height:24px;overflow:hidden;width:24px;}</style><style data-emotion="style-global 1hbd3g7">.style-1hbd3g7{height:250px;}</style><style data-emotion="style-global itrjxe">.style-itrjxe{background-color:var(--color-surface);border-radius:8px;padding:32px 56px;margin-bottom:24px;}@media (max-width: 769px){.style-itrjxe{padding:24px 32px;}}@media (max-width: 479px){.style-itrjxe{border-radius:0;margin:0 0 40px;padding:24px 16px;}}</style><style data-emotion="style-global 1cpeld6">.style-1cpeld6{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:var(--color-yellowContainerVariant);border-radius:8px;color:var(--color-highEmphasis);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:var(--font-size-body-2);font-weight:600;gap:8px;line-height:1.5;margin-bottom:16px;padding:16px;}</style><style data-emotion="style-global 6vbi9c">.style-6vbi9c{display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:24px;font-variation-settings:'FILL' 1,'wght' 500,'GRAD' 0,'opsz' 24;height:24px;overflow:hidden;width:24px;color:var(--color-yellowText);}</style><style data-emotion="style-global 8qb8m4">.style-8qb8m4{margin-bottom:48px;}</style><style data-emotion="style-global 183jbfu">.style-183jbfu{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:var(--color-green80);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:var(--font-size-body-2);font-weight:600;gap:4px;line-height:var(--line-height-body-dense);margin-bottom:16px;width:100%;}</style><style data-emotion="style-global 50qygk">.style-50qygk{color:var(--color-green80);-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:4px;overflow:hidden;}</style><style data-emotion="style-global 1gx6mxz">.style-1gx6mxz{display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:16px;font-variation-settings:'FILL' 1,'wght' 500,'GRAD' 0,'opsz' 24;height:16px;overflow:hidden;width:16px;}</style><style data-emotion="style-global 1e956jj">.style-1e956jj{display:block;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;}</style><style data-emotion="style-global epvm6">.style-epvm6{white-space:nowrap;}</style><style data-emotion="style-global 6su6fj">.style-6su6fj{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}</style><style data-emotion="style-global i43zkt">.style-i43zkt{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:var(--font-size-body-2);gap:0 8px;line-height:var(--line-height-body);width:100%;}</style><style data-emotion="style-global 17gh4w8">.style-17gh4w8{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:0 8px;width:calc(100% - 40px);}</style><style data-emotion="style-global mavs84">.style-mavs84{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:var(--color-highEmphasis);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:var(--font-size-body-1);font-weight:600;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;max-width:100%;}</style><style data-emotion="style-global kcbbwa">.style-kcbbwa{border-radius:50%;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;overflow:hidden;position:relative;width:24px;height:24px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;margin-right:8px;}.style-kcbbwa::before{background-color:var(--color-gray0);border-radius:50%;content:"";height:23px;left:50%;position:absolute;top:50%;-webkit-transform:translate(-50%, -50%);-moz-transform:translate(-50%, -50%);-ms-transform:translate(-50%, -50%);transform:translate(-50%, -50%);width:23px;}</style><style data-emotion="style-global 1wqqt93">.style-1wqqt93{display:block;height:24px;object-fit:contain;position:relative;width:24px;}</style><style data-emotion="style-global 15fzge">.style-15fzge{margin-left:4px;}</style><style data-emotion="style-global 1e7czb6">.style-1e7czb6{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;gap:0 4px;max-width:100%;}</style><style data-emotion="style-global 1o5v0u9">.style-1o5v0u9{color:var(--color-highEmphasis);font-weight:600;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:0 4px;max-width:calc(100% - 12px);}</style><style data-emotion="style-global bwd0ax">.style-bwd0ax{background-color:var(--color-gray0);border-radius:4px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:20px;width:20px;}</style><style data-emotion="style-global 8uhtka">.style-8uhtka{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}</style><style data-emotion="style-global wo2a1i">.style-wo2a1i{font-size:28px;font-weight:600;line-height:var(--line-height-headline);margin-top:8px;word-break:break-all;}</style><style data-emotion="style-global ee2v6o">.style-ee2v6o{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:4px;z-index:1;margin-top:16px;}</style><style data-emotion="style-global 1pmw9aj">.style-1pmw9aj{background-color:var(--color-surface-variant);border-radius:4px;color:var(--color-mediumEmphasis);display:block;font-size:var(--font-size-body-2);line-height:var(--line-height-body-dense);padding:0 6px;}.style-1pmw9aj:active{-webkit-text-decoration:none;text-decoration:none;background-color:var(--color-gray30);}@media (hover: hover) and (pointer: fine){.style-1pmw9aj:hover{-webkit-text-decoration:none;text-decoration:none;background-color:var(--color-gray30);}}@media (prefers-color-scheme: dark){.style-1pmw9aj:active{background-color:var(--color-gray70);}@media (hover: hover) and (pointer: fine){.style-1pmw9aj:hover{background-color:var(--color-gray70);}}}</style><style data-emotion="style-global 1npej5s">.style-1npej5s{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;font-size:var(--font-size-body-2);gap:0 8px;margin-top:8px;}</style><style data-emotion="style-global 3k9iaf">.style-3k9iaf{color:var(--color-mediumEmphasis);}</style><style data-emotion="style-global cbjizg">@media (prefers-color-scheme: dark){.style-cbjizg a{color:var(--color-green60);}.style-cbjizg a:visited{color:#c58af9;}}@media (prefers-color-scheme: dark){.style-cbjizg code{background-color:rgba(255, 255, 255, 0.08);}}@media (prefers-color-scheme: dark){.style-cbjizg blockquote::before{background-color:var(--color-gray80);}}@media (prefers-color-scheme: dark){.style-cbjizg .note.info{background:var(--color-green100);}}@media (prefers-color-scheme: dark){.style-cbjizg .note.warn{background:var(--color-yellow100);}}@media (prefers-color-scheme: dark){.style-cbjizg .note.alert{background:var(--color-red100);}}</style><style data-emotion="style-global r2q46l">.style-r2q46l{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-top:32px;gap:16px;}</style><style data-emotion="style-global 26pnip">.style-26pnip{border:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;background-color:transparent;border:none;border-radius:50%;color:var(--color-mediumEmphasis);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:44px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:0;width:44px;}.style-26pnip:active{background-color:var(--color-surface-variant);}@media (hover: hover) and (pointer: fine){.style-26pnip:hover{background-color:var(--color-surface-variant);}}</style><style data-emotion="style-global 6rai7j">.style-6rai7j{background-color:var(--color-background);bottom:0;box-shadow:0px 1px 4px rgba(0, 0, 0, 0.14),0px 0px 1px 1px var(--color-divider);display:none;height:calc(env(safe-area-inset-bottom, 0px) + 56px);-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding-bottom:env(safe-area-inset-bottom, 0px);position:fixed;width:100%;z-index:2000;}@media (max-width: 769px){.style-6rai7j{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="style-global 5jpx49">.style-5jpx49{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:space-evenly;-ms-flex-pack:space-evenly;-webkit-justify-content:space-evenly;justify-content:space-evenly;width:100%;}</style><style data-emotion="style-global 10h2231">.style-10h2231{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="style-global l8qm67">.style-l8qm67{color:var(--color-mediumEmphasis);cursor:pointer;font-size:var(--font-size-body-2);font-weight:bold;margin-left:8px;}</style><style data-emotion="style-global n5z2fc">.style-n5z2fc{color:var(--color-mediumEmphasis);font-size:14px;font-weight:bold;margin-left:8px;}</style><style data-emotion="style 1o9h1hg s47yiq">.style-1o9h1hg{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;background-color:transparent;border:none;color:var(--color-mediumEmphasis);cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:20px;height:32px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:0;width:32px;}.style-s47yiq{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;background-color:transparent;border:none;border-radius:50%;color:var(--color-mediumEmphasis);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:44px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:0;width:44px;}.style-s47yiq:active{background-color:var(--color-surface-variant);}@media (hover: hover) and (pointer: fine){.style-s47yiq:hover{background-color:var(--color-surface-variant);}}</style><style data-emotion="style-global 1cnt4b8">.style-1cnt4b8{background-color:var(--color-surface);}</style><style data-emotion="style-global ymuwam">.style-ymuwam{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;margin:auto;max-width:1656px;padding:8px 32px 0;width:100%;}@media (max-width: 769px){.style-ymuwam{padding:8px 16px 0;}}</style><style data-emotion="style-global b2g0bc">.style-b2g0bc{background-color:var(--color-green60);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:6px 10px;border-radius:4px;}.style-b2g0bc >svg{fill:var(--color-gray0);height:18px;}</style><style data-emotion="style-global 68a7fl">.style-68a7fl{margin-right:12px;position:relative;width:320px;}@media (max-width: 991px){.style-68a7fl{display:none;}}</style><style data-emotion="style-global 5l880u">.style-5l880u{display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:16px;font-variation-settings:'FILL' 0,'wght' 500,'GRAD' 0,'opsz' 24;height:16px;overflow:hidden;width:16px;color:var(--color-disabled);left:8px;position:absolute;top:50%;-webkit-transform:translateY(-50%);-moz-transform:translateY(-50%);-ms-transform:translateY(-50%);transform:translateY(-50%);}</style><style data-emotion="style-global 19agiue">.style-19agiue{background-color:var(--color-surface-variant);border:none;border-radius:8px;color:var(--color-highEmphasis);font-size:var(--font-size-body-1);line-height:var(--line-height-body);padding:4px 8px 4px 32px;width:100%;}.style-19agiue::-webkit-input-placeholder{color:var(--color-disabled);}.style-19agiue::-moz-placeholder{color:var(--color-disabled);}.style-19agiue:-ms-input-placeholder{color:var(--color-disabled);}.style-19agiue::placeholder{color:var(--color-disabled);}.style-19agiue::-webkit-search-cancel-button{-webkit-appearance:none;}</style><style data-emotion="style-global 6dnjxn">.style-6dnjxn{display:none;}@media (max-width: 991px){.style-6dnjxn{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:inherit;color:var(--color-mediumEmphasis);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:var(--font-size-body-1);-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:8px;}}.style-6dnjxn:active{color:var(--color-highEmphasis);}@media (hover: hover) and (pointer: fine){.style-6dnjxn:hover{color:var(--color-highEmphasis);}}</style><style data-emotion="style-global 1j8uoc8">.style-1j8uoc8{display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:24px;font-variation-settings:'FILL' 0,'wght' 500,'GRAD' 0,'opsz' 24;height:24px;overflow:hidden;width:24px;}</style><style data-emotion="style-global 1afofdy">.style-1afofdy{-webkit-transform:scale(0,0);-moz-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);position:absolute;}</style><style data-emotion="style-global 3kxxd">.style-3kxxd{position:relative;}@media (max-width: 479px){.style-3kxxd{position:static;}}</style><style data-emotion="style-global fmk4s6">.style-fmk4s6{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:var(--color-mediumEmphasis);background:none;padding:8px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;}.style-fmk4s6:active{color:var(--color-highEmphasis);}@media (hover: hover) and (pointer: fine){.style-fmk4s6:hover{color:var(--color-highEmphasis);}}</style><style data-emotion="style-global 16hp9px">.style-16hp9px{display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:24px;font-variation-settings:'FILL' 1,'wght' 500,'GRAD' 0,'opsz' 24;height:24px;overflow:hidden;width:24px;}</style><style data-emotion="style-global 1b9ff0k">.style-1b9ff0k{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;outline:none;padding:4px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;}@media (hover: hover) and (pointer: fine){.style-1b9ff0k:hover,.style-1b9ff0k:active{-webkit-filter:brightness(0.8);filter:brightness(0.8);}}@media (hover: none) and (any-pointer: coarse){.style-1b9ff0k:active{-webkit-filter:brightness(0.8);filter:brightness(0.8);}}</style><style data-emotion="style-global 135heiq">.style-135heiq{border-radius:50%;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;overflow:hidden;position:relative;width:32px;height:32px;}.style-135heiq::before{background-color:var(--color-gray0);border-radius:50%;content:"";height:31px;left:50%;position:absolute;top:50%;-webkit-transform:translate(-50%, -50%);-moz-transform:translate(-50%, -50%);-ms-transform:translate(-50%, -50%);transform:translate(-50%, -50%);width:31px;}</style><style data-emotion="style-global 1h6ertn">.style-1h6ertn{display:block;height:32px;object-fit:contain;position:relative;width:32px;}</style><style data-emotion="style-global komn7u">.style-komn7u{position:relative;margin-left:8px;}@media (max-width: 479px){.style-komn7u{position:static;}}@media (max-width: 769px){.style-komn7u{display:none;}}</style><style data-emotion="style-global osdn8o">.style-osdn8o{border-radius:8px;box-sizing:border-box;cursor:pointer;display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:14px;font-weight:600;line-height:1.8;margin-bottom:0;min-height:34px;min-width:64px;text-align:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;white-space:nowrap;-webkit-transition:.1s ease-out;transition:.1s ease-out;transition-property:background-color,border-color;background-color:var(--color-greenContainer);color:var(--color-onContainerText);padding:4px 16px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}.style-osdn8o:disabled{box-shadow:none;cursor:not-allowed;opacity:0.32;}.style-osdn8o:active{background-color:var(--color-greenContainerDim);}@media (hover: hover) and (pointer: fine){.style-osdn8o:hover{background-color:var(--color-greenContainerDim);}}</style><style data-emotion="style-global ib3052">.style-ib3052{display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:14px;font-variation-settings:'FILL' 1,'wght' 500,'GRAD' 0,'opsz' 24;height:14px;overflow:hidden;width:14px;margin-left:4px;}</style><style data-emotion="style-global mpp8vq">.style-mpp8vq{display:none;background-color:var(--color-surface);border-radius:8px;box-shadow:0px 1px 4px rgba(0, 0, 0, 0.14),0px 0px 1px 1px var(--color-divider);max-width:calc(100vw - 32px);min-width:260px;padding:12px 8px;position:absolute;top:56px;z-index:1000;right:0;}@media (max-width: 479px){.style-mpp8vq{right:16px;}}</style><style data-emotion="style-global 14mst66">.style-14mst66{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:inherit;border-radius:4px;color:var(--color-highEmphasis);cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:var(--font-size-body-1);line-height:var(--line-height-body);gap:8px;padding:4px 8px;width:100%;}.style-14mst66:hover{-webkit-text-decoration:none;text-decoration:none;}.style-14mst66:active{background-color:var(--color-surface-variant);-webkit-text-decoration:none;text-decoration:none;}@media (hover: hover) and (pointer: fine){.style-14mst66:hover{background-color:var(--color-surface-variant);}}</style><style data-emotion="style-global 1gx6mxz">.style-1gx6mxz{display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:16px;font-variation-settings:'FILL' 1,'wght' 500,'GRAD' 0,'opsz' 24;height:16px;overflow:hidden;width:16px;}</style><style data-emotion="style-global 1hyfx7x">.style-1hyfx7x{display:none;}</style><style data-emotion="style-global o5vjsc">.style-o5vjsc{display:none;margin:8px auto 0;width:calc(100vw - 32px);position:relative;}</style><style data-emotion="style-global 1wc0yei">.style-1wc0yei{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;background-color:var(--color-surface);}</style><style data-emotion="style-global ka8n9l">.style-ka8n9l{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:var(--font-size-body-2);font-weight:600;overflow-x:auto;max-width:1656px;padding:0 32px;width:100%;scrollbar-width:thin;scrollbar-color:var(--color-surface-variant) transparent;}.style-ka8n9l::-webkit-scrollbar{height:8px;}.style-ka8n9l::-webkit-scrollbar-track{background:transparent;}.style-ka8n9l::-webkit-scrollbar-thumb{background:var(--color-surface-variant);border-radius:4px;}@media (max-width: 769px){.style-ka8n9l{padding:0 16px;}}</style><style data-emotion="style-global ouma76">.style-ouma76{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:var(--color-mediumEmphasis);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;position:relative;white-space:nowrap;padding:8px 16px;}.style-ouma76:hover{color:var(--color-highEmphasis);-webkit-text-decoration:none;text-decoration:none;}</style><style data-emotion="style fv3lde">.style-fv3lde{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><script>window.frtn=window.frtn||function(){ (frtn.q=frtn.q||[]).push(arguments) };
frtn("init",{
service_id:"cova_248",
site_id:"site_134",
tag_id:"tag_283"
});
frtn("send","pageview");</script><script defer="" src="https://frtn.socdm.com/tags/insight.js" type="text/javascript"></script><link as="style" crossorigin="anonymous" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" onload="this.onload=null;this.rel=&#39;stylesheet&#39;" referrerpolicy="no-referrer" rel="preload" /><noscript><link crossorigin="anonymous" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" referrerpolicy="no-referrer" rel="stylesheet" /></noscript></head><body><div class="allWrapper"><div id="GlobalHeader-react-component-cdedb4f7-de08-454c-af4d-5cd65e7de477"><header class="style-1cnt4b8"><div class="style-ymuwam"><div class="style-fv3lde"><a href="/" aria-label="Qiita" class="style-b2g0bc"><svg viewBox="0 0 426.57 130" aria-hidden="true"><circle cx="167.08" cy="21.4" r="12.28"></circle><path d="M250.81 29.66h23.48v18.9h-23.48z"></path><path d="M300.76 105.26a22.23 22.23 0 01-6.26-.86 12.68 12.68 0 01-5.17-3 14.41 14.41 0 01-3.56-5.76 28 28 0 01-1.3-9.22V48.56h29.61v-18.9h-29.52V3.29h-20.17v83.34q0 11.16 2.83 18.27a27.71 27.71 0 007.7 11.2 26.86 26.86 0 0011.43 5.62 47.56 47.56 0 0012.34 1.53h15.16v-18zM0 61.7a58.6 58.6 0 015-24.21A62.26 62.26 0 0118.73 17.9 63.72 63.72 0 0139 4.78 64.93 64.93 0 0164 0a65 65 0 0124.85 4.78 64.24 64.24 0 0120.38 13.12A62 62 0 01123 37.49a58.6 58.6 0 015 24.21 58.34 58.34 0 01-4 21.46 62.8 62.8 0 01-10.91 18.16l11.1 11.1a10.3 10.3 0 010 14.52 10.29 10.29 0 01-14.64 0l-12.22-12.41a65 65 0 01-15.78 6.65 66.32 66.32 0 01-17.55 2.3 64.63 64.63 0 01-45.23-18A62.82 62.82 0 015 85.81 58.3 58.3 0 010 61.7zm21.64.08a43.13 43.13 0 0012.42 30.63 42.23 42.23 0 0013.43 9.09A41.31 41.31 0 0064 104.8a42 42 0 0030-12.39 42.37 42.37 0 009-13.64 43.43 43.43 0 003.3-17 43.77 43.77 0 00-3.3-17A41.7 41.7 0 0080.55 22 41.78 41.78 0 0064 18.68 41.31 41.31 0 0047.49 22a42.37 42.37 0 00-13.43 9.08 43.37 43.37 0 00-12.42 30.7zM331.89 78a47.59 47.59 0 013.3-17.73 43.22 43.22 0 019.34-14.47A44.25 44.25 0 01359 36a47.82 47.82 0 0118.81-3.58 42.72 42.72 0 019.26 1 46.5 46.5 0 018.22 2.58 40 40 0 017 3.84 44.39 44.39 0 015.71 4.63l1.22-9.47h17.35v85.83h-17.35l-1.17-9.42a42.54 42.54 0 01-5.84 4.67 43.11 43.11 0 01-7 3.79 44.86 44.86 0 01-8.17 2.59 43 43 0 01-9.22 1A47.94 47.94 0 01359 119.9a43.3 43.3 0 01-14.47-9.71 44.17 44.17 0 01-9.34-14.47 47 47 0 01-3.3-17.72zm20.27-.08a29.16 29.16 0 002.17 11.34 27 27 0 005.92 8.88 26.69 26.69 0 008.76 5.76 29.19 29.19 0 0021.44 0 26.11 26.11 0 008.72-5.76 27.57 27.57 0 005.88-8.84 29 29 0 002.16-11.38 28.62 28.62 0 00-2.16-11.22 26.57 26.57 0 00-5.93-8.8 27.68 27.68 0 00-19.51-7.9 28.29 28.29 0 00-10.77 2.05 26.19 26.19 0 00-8.71 5.75 27.08 27.08 0 00-5.84 8.8 28.94 28.94 0 00-2.13 11.31zm-194.97-30.5h19.78v73.54h-19.78zm49.25 0h19.78v73.54h-19.78z"></path><circle cx="216.33" cy="21.4" r="12.28"></circle></svg></a></div><div class="style-fv3lde"><form action="/search" method="get" role="search" aria-label="記事、質問を検索" class="style-68a7fl"><span class="material-symbols-outlined style-5l880u" aria-hidden="true">search</span><input type="search" autoComplete="off" placeholder="記事、質問を検索" value="" name="q" required="" class="style-19agiue"/></form><button class="style-6dnjxn"><span class="material-symbols-outlined style-1j8uoc8" aria-hidden="true">search</span><span class="style-1afofdy">記事、質問を検索</span></button><div class="style-3kxxd"><button tabindex="0" class="style-fmk4s6"><span class="material-symbols-outlined style-16hp9px" aria-hidden="true">notifications</span><span class="style-1afofdy">通知</span></button></div><div class="style-3kxxd"><button tabindex="0" class="style-1b9ff0k"><div class="style-135heiq"><img alt="ユーザーメニュー" height="32" loading="lazy" src="https://lh3.googleusercontent.com/a/ACg8ocJVPijPJ1D2gFKMC289nZRHFlTW9EYMMUEZS0MICcdi=s96-c" width="32" class="style-1h6ertn"/></div></button></div><div class="style-komn7u"><button font-size="14" class="style-osdn8o">投稿する<span class="material-symbols-outlined style-ib3052" aria-hidden="true">edit</span></button><div class="style-mpp8vq"><a href="/drafts/new" class="style-14mst66">記事を新規作成</a><a href="/questions/new" class="style-14mst66">質問を新規作成</a><a href="/drafts" class="style-14mst66">下書き一覧</a><a href="https://qiita.com/Qiita/items/32c79014509987541130" class="style-14mst66">記事をGitHubで管理<span class="material-symbols-outlined style-1gx6mxz" aria-hidden="true">open_in_new</span></a></div></div></div><div class="style-1hyfx7x"></div></div><form action="/search" method="get" role="search" aria-label="記事、質問を検索" class="style-o5vjsc"><span class="material-symbols-outlined style-5l880u" aria-hidden="true">search</span><input type="text" autoComplete="off" placeholder="記事、質問を検索" value="" name="q" required="" class="style-19agiue"/></form></header><nav class="style-1wc0yei"><div class="style-ka8n9l"><a href="/" class="style-ouma76">ホーム</a><a href="/timeline" class="style-ouma76">タイムライン</a><a href="/trend" class="style-ouma76">トレンド</a><a href="/question-feed" class="style-ouma76">質問</a><a href="/official-events" class="style-ouma76">公式イベント</a><a href="/official-columns" class="style-ouma76">公式コラム</a><a href="/opportunities" class="style-ouma76">募集</a><a href="/organizations" class="style-ouma76">Organization</a></div></nav></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="GlobalHeader" data-dom-id="GlobalHeader-react-component-cdedb4f7-de08-454c-af4d-5cd65e7de477">{"type":null,"isHideGlobalNavigation":false,"isShowTfaDownloadAlert":false}</script>
      
<div class="mainWrapper"><script type="application/ld+json">{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"/","name":"Qiita"}},{"@type":"ListItem","position":2,"item":{"@id":"/tags/deeplearning","name":"DeepLearning"}}]}</script><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","datePublished":"2017-11-28T18:21:06.000+09:00","dateModified":"2017-12-03T07:01:33.000+09:00","headline":"DeepLearning における会話モデル： Seq2Seq から VHRED まで","image":"https://qiita-user-contents.imgix.net/https%3A%2F%2Fcdn.qiita.com%2Fassets%2Fpublic%2Fadvent-calendar-ogp-background-f625e957b80c4bd8dd47b724be996090.jpg?ixlib=rb-4.0.0\u0026w=1200\u0026mark64=aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTkxNiZoPTMzNiZ0eHQ9RGVlcExlYXJuaW5nJTIwJUUzJTgxJUFCJUUzJTgxJThBJUUzJTgxJTkxJUUzJTgyJThCJUU0JUJDJTlBJUU4JUE5JUIxJUUzJTgzJUEyJUUzJTgzJTg3JUUzJTgzJUFCJUVGJUJDJTlBJTIwU2VxMlNlcSUyMCVFMyU4MSU4QiVFMyU4MiU4OSUyMFZIUkVEJTIwJUUzJTgxJUJFJUUzJTgxJUE3JnR4dC1jb2xvcj0lMjMzQTNDM0MmdHh0LWZvbnQ9SGlyYWdpbm8lMjBTYW5zJTIwVzYmdHh0LXNpemU9NTYmdHh0LWNsaXA9ZWxsaXBzaXMmdHh0LWFsaWduPWxlZnQlMkNtaWRkbGUmcz1jZDNjZTE2NTk1ZDkxYzU2NGU3MDlmZGUyMDVlYzViMw\u0026mark-x=142\u0026mark-y=151\u0026blend64=aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTcxNiZ0eHQ9JTQwaGFsaG9ybiUyMGluJTIwJUU2JUEwJUFBJUU1JUJDJThGJUU0JUJDJTlBJUU3JUE0JUJFTUlYSSZ0eHQtY29sb3I9JTIzM0EzQzNDJnR4dC1mb250PUhpcmFnaW5vJTIwU2FucyUyMFc2JnR4dC1zaXplPTMyJnR4dC1hbGlnbj1sZWZ0JTJDdG9wJnM9ODZiNjY1MzA0NDY0YTIxZDUwNTI3YmQ1MWE5YmIxOGU\u0026blend-x=142\u0026blend-y=491\u0026blend-mode=normal\u0026s=9fcf37782e42a812b75dbba705687de2","mainEntityOfPage":"https://qiita.com/halhorn/items/646d323ac457715866d4","author":{"@type":"Person","address":"","email":null,"identifier":"halhorn","name":"halhorn","image":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fprofile-images%2F1473695426?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=75\u0026s=f510227258c3f409a52db9dbfa75feb1","url":"https://qiita.com/halhorn","description":"会話 AI ロボット Romi を開発しています。","memberOf":[{"@type":"Organization","address":"〒150-6136 東京都渋谷区渋谷2-24-12 渋谷スクランブルスクエア 36F","legalName":"株式会社MIXI","image":"https://s3-ap-northeast-1.amazonaws.com/qiita-organization-image/a9b204def06e87e3156bfd962f459daa2b06ea3b/original.jpg?1649175523","logo":"https://s3-ap-northeast-1.amazonaws.com/qiita-organization-image/8dcc06a2c30582bb59e4e5a4fedd64c5d7bf0f85/original.jpg?1649176405","identifier":"mixi","description":"1997年の創業以来、SNS「mixi」やスマホアプリ「モンスターストライク」等、友人や家族といった親しい人と楽しむコミュニケーションサービスを提供してきました。これからも皆さまの生活をより豊かにすべく、新たな市場の創造に挑戦し続けます。\r\n※2022年10月1日、「株式会社ミクシィ」から「株式会社MIXI」へ商号が変わりました。"}]},"publisher":{"@type":"Organization","name":"Qiita","logo":{"@type":"ImageObject","url":"//cdn.qiita.com/assets/public/qiita-logo-c39ded593afa388e2e1ba435b110554e.png"}}}</script><img style="display:block;margin:0;padding:0;border:0;outline:0;width:0;height:0;line-height:0;" alt="" src="https://relay-dsp.ad-m.asia/dmp/sync/bizmatrix?pid=c3ed207b574cf11376&amp;d=x18o8hduaj&amp;uid=3733487" /><script type="application/json" id="js-react-on-rails-context">{"railsEnv":"production","inMailer":false,"i18nLocale":"ja","i18nDefaultLocale":"en","rorVersion":"13.4.0","rorPro":false,"href":"https://qiita.com/halhorn/items/646d323ac457715866d4","location":"/halhorn/items/646d323ac457715866d4","scheme":"https","host":"qiita.com","port":null,"pathname":"/halhorn/items/646d323ac457715866d4","search":null,"httpAcceptLanguage":"ja,en-US;q=0.9,en;q=0.8,zh-CN;q=0.7,zh;q=0.6","actionPath":"public/items#show","settings":{"analyticsTrackingId":"G-KEVS5DBRVN","tagManagerId":"GTM-W9W5TX4","assetsMap":{},"csrfToken":"qlw-jL2enrKe9AlwW5mhSsoZVZZB62o4yfd8bl70F3AMUEPMVQuR9gW7U4CdzWZi9ZGFiJEg7WXFvHQm1xVETg","locale":"ja","pushOne":{"accessToken":"4bc4bb057a0601d388a7b9e499ca2307e9eb4fc172804e310c59e511d7cac356","dialogImageUrl":"//cdn.qiita.com/assets/public/push_notification/image-qiitan-572179a3bbde375850422ea48b2b6272.png"},"textlint":{"workerUrl":"//cdn.qiita.com/assets/public/textlint-worker-0829171f0828e44da54b22ec4f2a17e3.min.js"}},"currentUser":{"defaultStockItemCategories":[{"encryptedId":"BAhJIh5TdG9ja0l0ZW1DYXRlZ29yeS0zOTYzOTY0BjoGRVQ=--ad07ff9e51094f260b870e3277347ffb2ec74acdd8d28ee0b805b7394bf5834e","isDefault":true,"isDestroyableByViewer":false,"isUpdatableByViewer":false,"name":"あとで読む","stockItemsCount":0}],"encryptedId":"BAhJIhNWaWV3ZXItMzczMzQ4NwY6BkVU--c2e1a5ebea5c1ce040510ad22a94d2d6e49de07151cb1e81305464db198503da","isBetaReleaseEnabled":false,"isDarkModeEnabled":true,"isEarlyAdapter":false,"isEmailConfirmed":true,"isEmailRegistered":true,"isExceededAiUsageLimit":false,"isFindyRegistered":false,"isPasswordRegistered":false,"isJobseeker":false,"isStaff":false,"isTeamOnly":false,"isTwitterLoginOnly":false,"monthlyPublicImageUploadableSizeLimit":104857600,"name":"","contribution":0,"originalId":3733487,"profileImageUrl":"https://lh3.googleusercontent.com/a/ACg8ocJVPijPJ1D2gFKMC289nZRHFlTW9EYMMUEZS0MICcdi=s96-c","profileImageUrlW48":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh3.googleusercontent.com%2Fa%2FACg8ocJVPijPJ1D2gFKMC289nZRHFlTW9EYMMUEZS0MICcdi%3Ds96-c?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=c696664d711a3e6342e78ee999cd1f2d","profileImageUrlW75":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh3.googleusercontent.com%2Fa%2FACg8ocJVPijPJ1D2gFKMC289nZRHFlTW9EYMMUEZS0MICcdi%3Ds96-c?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=75\u0026s=090182d19092504e19ad0f970c09e16e","remainingPublicImageUploadableSizeInCurrentMonth":104857600,"theme":"SYSTEM","unreadNotificationsCount":0,"urlName":"tian18351898629","webPushNotificationKey":"034468f00f894e34a4ba","organizations":{"edges":[],"totalCount":0}},"isLoggedIn":true,"recaptchaSiteKey":"6LfNkiQTAAAAAM3UGnSquBy2akTITGNMO_QDxMw6","errorIconPath":"//cdn.qiita.com/assets/icons/large/missing-profile-image-828ed5829a93fbb35746a6c0f7c8107e.png","additionalParams":{"query_parameters":{},"path_parameters":{"controller":"public/items","action":"show","user_id":"halhorn","type":"items","id":"646d323ac457715866d4"},"request_id":"cebc078e-2437-491f-b9ca-cee2f0199af1","user_id":3733487},"realms":[{"humanName":"Qiita","isCurrentRealm":true,"isQiita":true,"isQiitaTeam":false,"loggedInUser":{"profileImageUrl":"https://lh3.googleusercontent.com/a/ACg8ocJVPijPJ1D2gFKMC289nZRHFlTW9EYMMUEZS0MICcdi=s96-c","urlName":"tian18351898629"},"teamId":null,"url":"https://qiita.com/"}],"serverSide":false}</script>
<div id="PersonalArticlePage-react-component-b059084f-b34b-46ef-be87-c9b112fb6596"><div class="style-xgff9b"></div><div class=" style-qgd36e"><div class="style-yrmhnf"><div class="style-2pgeei"><div class="style-1duml6f"><svg width="64" height="64" viewBox="0 0 64 64" class="style-zq6eqh"><circle cx="32" cy="32" r="19" class="circle"></circle><path fill-rule="evenodd" clip-rule="evenodd" d="M30.5865 41.1378C31.3272 41.8221 32.6722 41.8319 33.4129 41.1574L33.4718 41.1037C38.5554 36.4713 41.8819 33.4401 41.746 29.6619C41.6875 28.0001 40.8396 26.4068 39.4654 25.4684C37.3743 24.0305 34.8841 24.4462 33.1038 25.7333C32.6936 26.0299 32.321 26.3728 31.9997 26.75C31.6783 26.3727 31.3056 26.03 30.8952 25.7338C29.115 24.4491 26.625 24.0385 24.5341 25.4684C23.1599 26.4068 22.312 28.0001 22.2535 29.6619C22.1268 33.4546 25.4697 36.4947 30.5865 41.1378ZM23.7526 29.7133C23.7953 28.5111 24.4144 27.3665 25.38 26.7071L25.3808 26.7065C27.1492 25.4973 29.4926 26.12 30.8579 27.7227L31.543 28.527C31.7826 28.8082 32.2169 28.8082 32.4565 28.527L33.1416 27.7227C34.5102 26.1161 36.8515 25.4914 38.6155 26.7044L38.6195 26.7071C39.5855 27.3667 40.2046 28.512 40.2469 29.7146L40.247 29.7158C40.2969 31.1036 39.73 32.4647 38.3804 34.1435C37.0091 35.8492 34.9938 37.6874 32.403 40.0483C32.3636 40.0842 32.2314 40.1587 32.0103 40.1572C31.7898 40.1556 31.6512 40.0792 31.6044 40.036L31.5995 40.0315L31.5945 40.027C29.0048 37.677 26.9919 35.8432 25.6225 34.1408C24.2752 32.4657 23.7066 31.1034 23.7526 29.7133Z" class="heart-stroke"></path><path d="M33.4129 41.1574C32.6722 41.8319 31.3272 41.8221 30.5865 41.1378C25.4697 36.4947 22.1268 33.4546 22.2535 29.6619C22.312 28.0001 23.1599 26.4068 24.5341 25.4684C27.1071 23.7089 30.2844 24.7363 31.9997 26.75C33.7151 24.7363 36.8924 23.6991 39.4654 25.4684C40.8396 26.4068 41.6875 28.0001 41.746 29.6619C41.8824 33.4546 38.5297 36.4947 33.4129 41.1574Z" class="heart-fill"></path><g class="particles" style="--rotate:30deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g><g class="particles" style="--rotate:102deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g><g class="particles" style="--rotate:174deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g><g class="particles" style="--rotate:246deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g><g class="particles" style="--rotate:318deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g></svg><button aria-label="いいね" title="いいね" class="style-10ttvi6"></button></div><a href="/halhorn/items/646d323ac457715866d4/likers" aria-label="156いいね" class="style-klat6i">156</a></div><div class="style-krcte5"><button aria-label="ストック" title="ストック" class="style-1xwfn2v"><svg class="style-100tu0r" height="32" viewBox="0 0 32 32" width="32"><path d="M6.5 5H25.5C25.7761 5 26 5.22386 26 5.5V6.5C26 6.77614 25.7761 7 25.5 7H6.5C6.22386 7 6 6.77614 6 6.5V5.5C6 5.22386 6.22386 5 6.5 5ZM24 19V11.5C24 11.3674 23.9473 11.2402 23.8536 11.1464C23.7598 11.0527 23.6326 11 23.5 11H8.5C8.36739 11 8.24021 11.0527 8.14645 11.1464C8.05268 11.2402 8 11.3674 8 11.5V19C8 20.8565 8.7375 22.637 10.0503 23.9497C11.363 25.2625 13.1435 26 15 26H17C18.8565 26 20.637 25.2625 21.9497 23.9497C23.2625 22.637 24 20.8565 24 19ZM6.58579 9.58579C6.96086 9.21071 7.46957 9 8 9H24C24.5304 9 25.0391 9.21071 25.4142 9.58579C25.7893 9.96086 26 10.4696 26 11V19C26 21.3869 25.0518 23.6761 23.364 25.364C21.6761 27.0518 19.3869 28 17 28H15C12.6131 28 10.3239 27.0518 8.63604 25.364C6.94821 23.6761 6 21.3869 6 19V11C6 10.4696 6.21071 9.96086 6.58579 9.58579Z"></path></svg></button><span class="style-5ji4gp">136</span></div><button aria-label="X(Twitter)でシェア" title="ポスト" class="style-1n9ulpr" type="button"><svg class="style-1b1cd5z" viewBox="0 0 20 20"><path d="m11.68 8.62 6.55-7.62h-1.55l-5.69 6.62-4.55-6.62h-5.25l6.88 10.01-6.88 7.99h1.55l6.01-6.99 4.8 6.99h5.24l-7.13-10.38zm-2.13 2.47-.7-1-5.54-7.92h2.39l4.47 6.4.7 1 5.82 8.32h-2.39l-4.75-6.79z"></path></svg></button><button aria-label="facebookでシェア" title="Facebookでシェア" class="style-1n9ulpr" type="button"><svg class="style-157xj1g" height="20" viewBox="0 0 20 20" width="20"><path d="m20 10c0-5.52-4.48-10-10-10s-10 4.48-10 10c0 4.99 3.66 9.13 8.44 9.88v-6.99h-2.54v-2.89h2.54v-2.2c0-2.51 1.49-3.89 3.78-3.89 1.09 0 2.24.2 2.24.2v2.46h-1.26c-1.24 0-1.63.77-1.63 1.56v1.88h2.77l-.44 2.89h-2.33v6.99c4.78-.75 8.44-4.89 8.44-9.88z"></path><path d="m13.89 12.89.44-2.89h-2.77v-1.88c0-.79.39-1.56 1.63-1.56h1.26v-2.46s-1.14-.2-2.24-.2c-2.28 0-3.78 1.38-3.78 3.89v2.2h-2.54v2.89h2.54v6.99c.51.08 1.03.12 1.56.12s1.05-.04 1.56-.12v-6.99h2.33z" fill="#ffffff"></path></svg></button><a aria-label="はてなブックマーク" class="style-1o9h1hg" href="https://b.hatena.ne.jp/entry/s/qiita.com/halhorn/items/646d323ac457715866d4" rel="noopener noreferrer" target="_blank" title="はてなブックマーク"><svg class="style-1uu7u3w" viewBox="0 0 20 20"><rect height="18" rx="3.67" width="18" x="1" y="1" fill="#1d9bf0"></rect><g fill="#ffffff"><path d="m11.01 10.29c-.33-.36-.78-.57-1.36-.61.52-.14.89-.35 1.13-.62s.35-.64.35-1.11c0-.37-.08-.69-.24-.97s-.39-.5-.69-.67c-.26-.15-.58-.25-.94-.31-.37-.06-1.01-.09-1.93-.09h-2.24v8.18h2.31c.93 0 1.6-.03 2.01-.09s.75-.17 1.03-.32c.34-.18.61-.44.79-.77s.28-.72.28-1.15c0-.61-.16-1.09-.49-1.46zm-3.85-2.57h.48c.55 0 .93.06 1.12.19s.29.34.29.65-.1.5-.31.63c-.21.12-.58.18-1.13.18h-.45v-1.64zm1.9 4.69c-.22.13-.59.2-1.12.2h-.78v-1.79h.81c.54 0 .91.07 1.11.2.2.14.3.38.3.72 0 .31-.11.53-.33.66z"></path><path d="m13.87 12.02c-.57 0-1.04.46-1.04 1.04s.46 1.04 1.04 1.04 1.04-.46 1.04-1.04-.46-1.04-1.04-1.04z"></path><path d="m12.97 5.91h1.8v5.45h-1.8z"></path></g></svg></a><div class="style-79elbk"><button type="button" aria-label="オプションを開く" class="style-rvwogb"><span class="material-symbols-outlined style-1j8uoc8" aria-hidden="true">more_horiz</span></button></div></div><div class="p-items_options"><div><div class="style-1hbd3g7"></div></div><div class="p-items_toc"><div class="style-1hbd3g7"></div></div></div><div class="p-items_main"><div class="style-itrjxe"><div class="style-1cpeld6"><span class="material-symbols-outlined style-6vbi9c" aria-hidden="true">info</span><p>この記事は最終更新日から5年以上が経過しています。</p></div><div class="style-8qb8m4"><div class="style-183jbfu"><a href="/advent-calendar/2017/mixi" class="style-50qygk"><span class="material-symbols-outlined style-1gx6mxz" aria-hidden="true">park</span><span class="style-1e956jj">mixiグループ</span><span class="style-epvm6">Advent Calendar <!-- -->2017</span></a><span class="style-6su6fj">3日目</span></div><div data-logly-image="true" class="style-i43zkt"><div class="style-17gh4w8"><a href="/halhorn" class="style-mavs84"><div class="style-kcbbwa"><img height="24" loading="lazy" src="https://qiita-user-profile-images.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fprofile-images%2F1473695426?ixlib=rb-4.0.0&amp;auto=compress%2Cformat&amp;lossless=0&amp;w=48&amp;s=1705e4b22169c3dd535fe8e150abf298" width="24" class="style-1wqqt93"/></div>@<!-- -->halhorn<span class="style-15fzge">(<!-- -->Harumitsu Nobuta<!-- -->)</span></a><span class="style-1e7czb6"><span>in</span><a href="/organizations/mixi" class="style-1o5v0u9"><img src="https://s3-ap-northeast-1.amazonaws.com/qiita-organization-image/8dcc06a2c30582bb59e4e5a4fedd64c5d7bf0f85/original.jpg?1649176405" alt="" height="20" width="20" class="style-bwd0ax"/><span class="style-8uhtka">株式会社MIXI</span></a></span></div></div><h1 data-logly-title="true" class="style-wo2a1i">DeepLearning における会話モデル： Seq2Seq から VHRED まで</h1><ul class="style-ee2v6o"><li><a href="/tags/nlp" class="style-1pmw9aj">NLP</a></li><li><a href="/tags/deeplearning" class="style-1pmw9aj">DeepLearning</a></li><li><a href="/tags/vhred" class="style-1pmw9aj">VHRED</a></li></ul><div class="style-1npej5s"><span class="style-3k9iaf">最終更新日 <time dateTime="2017-12-02T22:01:33Z">2017年12月02日</time></span><span class="style-3k9iaf">投稿日 <!-- -->2017年12月02日</span></div></div><section class="it-MdContent darkMode"><div id="personal-public-article-body"><div class="mdContent-inner style-cbjizg"><p>こんにちは。 DeepLearning で対話ロボットを作ろうとしているインコです。 <br>
この記事は mixi Advent Calendar 2017 の 12/03 の記事です。</p>

<h1>
<span id="概要" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>概要</h1>

<p>近年対話モデルとして DeepLearning を用いた End to End のアプローチが盛んに行われています。<br>
この記事ではこれらに用いられるモデルとして一問一答に使われる Seq2Seq から出発して、複数発話コンテキストを扱いベイズ的なアプローチを組み込んだ VHRED を理解することをゴールとします。<br>
<a href="https://camo.qiitausercontent.com/e02332e7f2a0a331670237ff83b2bc2059a3de26/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f64663061336264362d396564302d326665612d333139382d3163353435373563326334652e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fdf0a3bd6-9ed0-2fea-3198-1c54575c2c4e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a8a60d3126f604284b0c4569467bfca2" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/df0a3bd6-9ed0-2fea-3198-1c54575c2c4e.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fdf0a3bd6-9ed0-2fea-3198-1c54575c2c4e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=9435aa20ad2af5bf84078d69c238ef95 1x" loading="lazy"></a></p>

<h1>
<span id="会話モデルのもろもろ" class="fragment"></span><a href="#%E4%BC%9A%E8%A9%B1%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%82%82%E3%82%8D%E3%82%82%E3%82%8D"><i class="fa fa-link"></i></a>会話モデルのもろもろ</h1>

<h2>
<span id="seq2seq" class="fragment"></span><a href="#seq2seq"><i class="fa fa-link"></i></a>Seq2Seq</h2>

<p><a href="https://arxiv.org/pdf/1506.05869.pdf" class="autolink" rel="nofollow noopener" target="_blank">https://arxiv.org/pdf/1506.05869.pdf</a><br>
<a href="https://camo.qiitausercontent.com/587c06a1455d43087135887ca8049399f5b6ba41/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f33626435313138392d663638352d313332382d373730642d6330616464376636646238312e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F3bd51189-f685-1328-770d-c0add7f6db81.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=e59f6549665c251c868634a9656033f2" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/3bd51189-f685-1328-770d-c0add7f6db81.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F3bd51189-f685-1328-770d-c0add7f6db81.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=0b0979faddae658fdd6103bc4d7d3787 1x" loading="lazy"></a></p>

<p>DeepLearning で対話！と言ったときにまず出てくる基本的なモデルが Sequence to Sequence こと Seq2Seq です。<br>
これは発話・応答のシーケンスのペアを学習させることで、発話から応答を生成するモデルです。<br>
<a href="https://www.tensorflow.org/tutorials/seq2seq/" rel="nofollow noopener" target="_blank">tensorflow 上</a>にも実装があります。</p>

<p>対話モデル以外にも様々な方面に応用されていますが、特に入力を日本語の文、出力を英語の文などとして翻訳モデルとして盛んに使われています。 Google 翻訳が 2016/11 に劇的に精度向上したのが<a href="http://trendy.nikkeibp.co.jp/atcl/column/16/032300106/090700005/" rel="nofollow noopener" target="_blank">話題</a>になりましたが、この<a href="https://research.googleblog.com/2017/07/building-your-own-neural-machine.html" rel="nofollow noopener" target="_blank"> Google 翻訳にも Seq2Seq が使われている</a>ようです。</p>

<h3>
<span id="構造" class="fragment"></span><a href="#%E6%A7%8B%E9%80%A0"><i class="fa fa-link"></i></a>構造</h3>

<p>LSTM などの RNN を用いたネットワークで２つの部分から構成されます<br>
（LSTM?? RNN?? という方は<a href="https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca" id="reference-dec2ed67fcab51f8552b">LSTMネットワークの概要</a> を読むのがおすすめです）</p>

<ul>
<li>
<strong>Encoder RNN</strong>: （図の Context） 人間からの問いかけの文章を単語などトークンに区切って渡します</li>
<li>
<strong>Decoder RNN</strong>: (図の Reply) システムからの応答を単語などトークン毎に生成します</li>
</ul>

<p>Encoder RNN は図のトークン A, B, C を入力として受け取ったあとの final state を Decoder RNN の initial state として渡します。<br>
この Encoder RNN の final state は thought vector と呼ばれており、 A, B, C という文章全体の情報を持つベクトルとなるとされています。</p>

<p>Seq2Seq をうまく学習させるための研究は盛んに行われており、 LSTM を多層にする、 Encoder RNN を bidirectional RNN にする、 <a href="https://qiita.com/halhorn/items/614f8fe1ec7663e04bea" id="reference-7db01c735ce2b9a9dc20">Attention Mechanism (日本語解説)</a> を使うなど様々な性能改善手法があります。</p>

<h4>
<span id="もうすこし具体的に" class="fragment"></span><a href="#%E3%82%82%E3%81%86%E3%81%99%E3%81%93%E3%81%97%E5%85%B7%E4%BD%93%E7%9A%84%E3%81%AB"><i class="fa fa-link"></i></a>もうすこし具体的に</h4>

<p>これだけだと抽象的でわかりづらいのでもう少し具体的なフローを説明します。<br>
「インコは可愛いね」 という文を Seq2Seq に入力して、 「可愛いよね」 が生成されるまでの過程は以下のようになります。<br>
<a href="https://camo.qiitausercontent.com/2775c0917ff4e0cca6ef5b96cfb28de50330d6c3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f36653865636338342d346364362d366332372d363662372d6663383065346138306635332e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F6e8ecc84-4cd6-6c27-66b7-fc80e4a80f53.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=4d896cf1d11766bec1716207c2647a89" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/6e8ecc84-4cd6-6c27-66b7-fc80e4a80f53.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F6e8ecc84-4cd6-6c27-66b7-fc80e4a80f53.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=84022675d50e7e1aa19bc5262a984ff7 1x" loading="lazy"></a></p>

<ol>
<li>Encoding

<ol>
<li>
<font color="orange">Tokenize:</font> 文章を単語等（token と呼びます）毎に分割し、 token 毎の ID に変換します。</li>
<li>
<font color="orange">Embedding:</font> ID から、その token を表す分散表現ベクトルに変換します。

<ul>
<li>
<a href="https://deepage.net/bigdata/machine_learning/2016/09/02/word2vec_power_of_word_vector.html" rel="nofollow noopener" target="_blank">Word2Vec</a> が有名ですね。</li>
<li>私のチームではこれを文字単位で行う <a href="http://alpha.mixi.co.jp/entry/2017/10/12/154825" rel="nofollow noopener" target="_blank">Char2Vec なども試して</a>います。</li>
<li>word2vec を使わなくとも、 token ID と一対一対応する適当な正規分布からサンプルしたベクトルを入れておけば OK です。</li>
</ul>
</li>
<li>
<font color="orange">Encoder RNN:</font> ベクトルを順番に RNN に入力していきます。

<ul>
<li>vec1 を RNN に入力して hidden state (横矢印)を出力。この hidden state と次の入力 vec2 をまた RNN に入力してまた hidden state を出力・・を繰り返します。</li>
<li>最後の vec4 を入れたときの hidden state を final state としてとっておきます。</li>
<li>この final state が thought vector と呼ばれ、「インコは可愛いね」という文の意味のようなものを表すベクトルとなっています。</li>
<li>Encoder とはつまり、「インコ可愛いね」という文（の ID 列）を thought vector にエンコードするものなわけです</li>
</ul>
</li>
</ol>
</li>
<li>Decoding

<ol>
<li>
<font color="orange">Decoder RNN:</font> Encoder RNN の final state (thought vector) から、各 token の生成確率を出力していきます

<ol>
<li>final state を Decoder RNN の initial state ととして設定し、特別なシンボル <code>&lt;GO&gt;</code> の Embedding を入力</li>
<li>RNN の隠れ層に全結合層等を噛まして、 token ID ごとの生成確率を出力。

<ul>
<li>例えば <code>[0.1, 0.001, 0.3, ..]</code> なら ID:0 は10%、ID:1は0.1%・・といった具合</li>
</ul>
</li>
</ol>
</li>
<li>
<font color="orange">Sampling:</font> 生成確率にもとづいて token をランダムに選びます

<ul>
<li>より精度の良い生成を行うにはここでビームサーチを行います</li>
</ul>
</li>
<li>
<font color="orange">Embedding:</font> 2で選ばれた token を Embedding して Decoder RNN
への次の入力とします。</li>
<li>
<font color="orange">Detokenize:</font> 1-3 を繰り返し、2で得られた token を文字列に直します</li>
</ol>
</li>
</ol>

<p>このようにして、 Seq2Seq はインコの可愛さに同意することが可能になります。<br>
ここで最終的に説明したい VHRED への伏線として、　2.2 で次の<strong>token (単語等)</strong>を選ぶときに（重み付き）ランダムサンプリングをしていることを覚えておいて下さい。<br>
ニューラルネットというとランダム性無く決定論的に生成を行うイメージがありますが、 Seq2Seq ではこのように単語などの並びというレベルでは生成される文にランダム性をもたせることができます。</p>

<h3>
<span id="できること" class="fragment"></span><a href="#%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8"><i class="fa fa-link"></i></a>できること</h3>

<p>元論文では映画のセリフを学習データとして使うことで以下のように様々な問に答えるモデルができたとしています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><code>Human: who is skywalker ?
Machine: he is a hero .
Human: who is bill clinton ?
Machine: he ’s a billionaire .
Human: is sky blue or black ?
Machine: blue .
Human: does a cat have a tail ?
Machine: yes .
Human: does a cat have a wing ?
Machine: no
Human: can a cat fly ?
Machine: no .
...
</code></pre></div></div>

<p>ただし、このモデルは直前の会話のみを Encoder RNN に渡す仕組みですので、それより前の発言から次の発言を生成することはできません。つまり一問一答です。</p>

<h2>
<span id="hred" class="fragment"></span><a href="#hred"><i class="fa fa-link"></i></a>HRED</h2>

<p><a href="https://arxiv.org/pdf/1507.04808.pdf" class="autolink" rel="nofollow noopener" target="_blank">https://arxiv.org/pdf/1507.04808.pdf</a><br>
実装： <a href="https://github.com/julianser/hed-dlg-truncated" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/julianser/hed-dlg-truncated</a><br>
<a href="https://camo.qiitausercontent.com/65bd301a355c4ee9aea826d2492fc82b224be996/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f36383735356131372d616663362d396661372d383062322d6234333234626162333938312e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F68755a17-afc6-9fa7-80b2-b4324bab3981.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=ab14779eb884e8797b1aafe663d769da" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/68755a17-afc6-9fa7-80b2-b4324bab3981.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F68755a17-afc6-9fa7-80b2-b4324bab3981.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=375496f52e403bdd255f9df67eaf9170 1x" loading="lazy"></a></p>

<p>Hierarchical Recurrent Encoder-Decoder の略です。<br>
Seq2Seq は一問一答ですが、これを過去の n-1 個の発話から次の n 個目の発話を推測するようにしたのが HRED です。</p>

<p>Seq2Seq では例えば</p>

<ul>
<li>システム：「インコ好きだよね？」</li>
<li>ユーザー：「うん」</li>
<li>システム：＜次の答え＞</li>
</ul>

<p>の次の答えが「うん」のみから生成されるため、おそらくインコに関する話題が次生成されることはありません。<br>
HRED では過去 n-1 個の発話から次の発話を生成するため、例えば「インコかわいいよねわかる。」みたいな発話を生成できる可能性があります。</p>

<h3>
<span id="構造-1" class="fragment"></span><a href="#%E6%A7%8B%E9%80%A0-1"><i class="fa fa-link"></i></a>構造</h3>

<p>Seq2Seq は Encoder RNN, Decoder RNN の2段構成でしたが、 HRED は Encoder RNN, Context RNN, Decoder RNN の3段構成です。</p>

<ol>
<li>Encoder RNN: 一つ一つの文章（会話なら過去の一つ一つの発言）をそれを表すベクトルに変換する</li>
<li>Context RNN: Encoder のまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する</li>
<li>Decoder RNN: Context RNN の情報から応答を生成する</li>
</ol>

<p>2 の Context RNN というレイヤーがあることによって、過去の発話の履歴を加味した返答をできるようになっているということですね。</p>

<h2>
<span id="vae" class="fragment"></span><a href="#vae"><i class="fa fa-link"></i></a>VAE</h2>

<p>VAE は対話モデルではないのですが、最終的に説明をしたい VHRED を数学的に理解する上で重要なモデルですので説明をします。</p>

<p><a href="https://arxiv.org/pdf/1312.6114.pdf" class="autolink" rel="nofollow noopener" target="_blank">https://arxiv.org/pdf/1312.6114.pdf</a><br>
<a href="https://camo.qiitausercontent.com/38dce7c2a04f38d807194c1d6392fd38fe29b62c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f63353633326332622d316534302d303833382d656530342d3437316536366133346164662e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fc5632c2b-1e40-0838-ee04-471e66a34adf.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=ec227451df37789beb767c757553f9e4" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/c5632c2b-1e40-0838-ee04-471e66a34adf.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fc5632c2b-1e40-0838-ee04-471e66a34adf.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=bcbb71a754415cb426ffac69fed5ba87 1x" loading="lazy"></a></p>

<p>標準正規分布からサンプリングした潜在変数 z から画像等データを生成することのできるモデルです。<br>
<a href="https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24" id="reference-554853f8b4d6cb157abf">Variational Autoencoder徹底解説</a> がとても詳しくわかりやすく書かれておりおすすめです。</p>

<h3>
<span id="vae-ができること" class="fragment"></span><a href="#vae-%E3%81%8C%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8"><i class="fa fa-link"></i></a>VAE ができること</h3>

<p>標準正規分布 $\mathscr{N}(0, I)$ から適当な潜在変数 z をサンプリングして VAE に入力することで、学習データをうまく補完したようなデータを生成できます。<br>
<a href="https://camo.qiitausercontent.com/caa898648b2e5524af4b6ec69281a6c98ff3bb88/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f38313236326233362d383538382d663263642d663933642d3833636263356131366231642e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F81262b36-8588-f2cd-f93d-83cbc5a16b1d.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=29a47ad1b1d08169e89193e668d60146" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/81262b36-8588-f2cd-f93d-83cbc5a16b1d.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F81262b36-8588-f2cd-f93d-83cbc5a16b1d.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=89ce22dda75aecc2cb0d8c73f924097a 1x" loading="lazy"></a></p>

<p><a href="https://camo.qiitausercontent.com/d201e93374732ae33e37ae1a9088fe3beb11f928/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f62653866616232632d663165352d343837632d396232652d3633316537643761333364652e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fbe8fab2c-f1e5-487c-9b2e-631e7d7a33de.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=0ed7df958a47135202190d0f483cfb7b" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/be8fab2c-f1e5-487c-9b2e-631e7d7a33de.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fbe8fab2c-f1e5-487c-9b2e-631e7d7a33de.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=939ad53510fc60fb95390617c6358c69 1x" loading="lazy"></a><br>
この例では画像を生成していますが、言語の生成への応用なども研究されています。</p>

<h3>
<span id="vae-の数学的な考え方" class="fragment"></span><a href="#vae-%E3%81%AE%E6%95%B0%E5%AD%A6%E7%9A%84%E3%81%AA%E8%80%83%E3%81%88%E6%96%B9"><i class="fa fa-link"></i></a>VAE の数学的な考え方</h3>

<p><a href="https://arxiv.org/pdf/1312.6114.pdf" rel="nofollow noopener" target="_blank">Auto-Encoding Variational Bayes</a> で提案された、潜在変数<br>
 z からデータ x が生成される場合の汎用な数学的モデルがまずあり、 VAE はその例として書かれているものです。</p>

<h4>
<span id="auto-encoding-variational-bayes" class="fragment"></span><a href="#auto-encoding-variational-bayes"><i class="fa fa-link"></i></a>Auto-Encoding Variational Bayes</h4>

<p>まずは VAE の元になっている数学的モデルの説明をします。<br>
潜在変数 z の事前分布 $P_\theta(z)$ があり、 $P_\theta(x|z)$ によって x が生成されるとします。<br>
この時点では $P_\theta(z)$, $P_\theta(x|z)$ はその確率密度関数が $\theta, z$ 両方に関してほぼ全体で微分可能という仮定はありますが、とくにそれらが正規分布だとかいう仮定はありません。</p>

<p>このようなモデルでの対数尤度を最大化することを考えます。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>L=\sum_xlogP_\theta(x)
</code></pre></div></div>

<p>目的は上の対数尤度 L を最大化する $\theta$ を見つけることになります。<br>
L を最大化するには各 $logP_\theta(x)$ を最大化すれば良いです。</p>

<p><a href="https://qiita.com/halhorn/private/646d323ac457715866d4#%E4%BB%98%E9%8C%B21-vae-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA">付録1、 Lower Bound の導出</a>より、適当な分布 $Q_\phi(z|x)$ に対して以下が言えます。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(x) \geqq -KL[Q_\phi(z|x)||P_\theta(z)] + E_{Q_\phi(z|x)}[logP_\theta(x|z)]
</code></pre></div></div>

<ul>
<li>$Q_\phi(z|x)$ は事後分布 $P_\theta(z|x)$ の近似（付録1参照）</li>
<li>$KL[Q||P]$: <a href="https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%AB%E3%83%90%E3%83%83%E3%82%AF%E3%83%BB%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%BC%E6%83%85%E5%A0%B1%E9%87%8F" rel="nofollow noopener" target="_blank">Kullback-Leibler divergence</a> つまり Q, P ２つの分布の差異。非負。</li>
<li>$E_Q(P)$: 確率分布QでのPの期待値。</li>
</ul>

<p>この右辺を Lower Bound と呼び、この右辺を最大化することによって $logP_\theta$ を最大化します。<br>
つまり、 KL を小さくして E を大きくすれば良いわけです。</p>

<h4>
<span id="vae-1" class="fragment"></span><a href="#vae-1"><i class="fa fa-link"></i></a>VAE</h4>

<p>ここで、　P, Q の条件付き確率に対して、条件を入力としてその確率分布を出力するニューラルネットワークを使うことを考えます。<br>
これが VAE です。<br>
VAE では $P_\theta(z)$ は標準正規分布 $\mathscr{N}(0,I)$ を仮定します。</p>

<ul>
<li>$Q_\phi(z|x)$: <strong>Encoder</strong>

<ul>
<li>x を入力として z の分布を出力

<ul>
<li>VAE では正規分布を仮定</li>
<li>$\mathscr{N}(\mu(x), \sigma(x))$ の $\mu, \sigma$ を出力するニューラルネット</li>
</ul>
</li>
</ul>
</li>
<li>$logP_\theta(x|z)$: <strong>Decoder</strong>

<ul>
<li>z を入力として x の分布を出力

<ul>
<li>VAE では正規分布もしくはベルヌーイ分布を仮定</li>
</ul>
</li>
</ul>
</li>
</ul>

<p>右辺第一項の $KL[Q_\phi(z|x)||P_\theta(z)]$ は Encoder をできるだけ $P_\theta(z)$ に近づければ小さくなります。<br>
$P_\theta(z)$ は標準正規分布としたので、 $logP_\theta(x)$ を大きくするには $Q_\phi(z|x)$ (Encoder) を標準正規分布に近づくよう学習させればよいことになります。</p>

<p>右辺第二項の $E_{Q_\phi(z|x)}[logP_\theta(x|z)]$ は $x$ を Encoder への入力として $z$ を生成し、その $z$ を更に Decoder に入力して $x^\prime$ を出力するニューラルネットとみなせます。ですのでこの出力 $x^\prime$ が真の分布に近づくよう、大元の入力 $x$ との誤差を小さくするよう学習させることで $logP_\theta(x)$ を大きくできます。</p>

<p><a href="https://camo.qiitausercontent.com/b28aa4e7c970a1f8e355bbd0eddb0e1093ceb0f8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f62323431383136642d666663612d363135612d396232362d6235343966613663646335662e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fb241816d-ffca-615a-9b26-b549fa6cdc5f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=41f595eeeda8ab2b884147e0f153d1ef" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/b241816d-ffca-615a-9b26-b549fa6cdc5f.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fb241816d-ffca-615a-9b26-b549fa6cdc5f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=d6435c0ee033e6a59588bbd4d48e556c 1x" loading="lazy"></a></p>

<h1>
<span id="vhred" class="fragment"></span><a href="#vhred"><i class="fa fa-link"></i></a>VHRED</h1>

<p><a href="https://arxiv.org/pdf/1605.06069.pdf" class="autolink" rel="nofollow noopener" target="_blank">https://arxiv.org/pdf/1605.06069.pdf</a><br>
実装： <a href="https://github.com/julianser/hed-dlg-truncated" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/julianser/hed-dlg-truncated</a><br>
<a href="https://camo.qiitausercontent.com/f41ed0745eb83b0278272a70f3c8d89370cd3abe/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f32663132633863332d323837662d313535332d383737612d3337366437613466336666302e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F2f12c8c3-287f-1553-877a-376d7a4f3ff0.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=cda12235bf193815c9508472b3cd01d9" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/2f12c8c3-287f-1553-877a-376d7a4f3ff0.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F2f12c8c3-287f-1553-877a-376d7a4f3ff0.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=e14ab6ebb7072510980b2baaf9e63003 1x" loading="lazy"></a><br>
上のネットワーク図を見て分かる通り、 HRED に対して VAE の潜在変数 z を組み合わせたモデルです。</p>

<h2>
<span id="vhred-でできること" class="fragment"></span><a href="#vhred-%E3%81%A7%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8"><i class="fa fa-link"></i></a>VHRED でできること</h2>

<p>HRED と同じく過去の n-1 個の発話を与えられて、 n 個目の発話を生成します。<br>
ただし、 HRED は対話学習において以下の問題を持っておりこれを解決することを目的にしています</p>

<ul>
<li>HRED は確率的な多様性が字面にしかなく、会話の「流れ」のようなロングタームな多様性が無い。

<ul>
<li> Encoder RNN, Context RNN, Decoder RNN のうち確率的な処理が Decoder RNN の次ステップの単語を生成する部分にしか無いから。</li>
<li> これによって、同じコンテキスト（発話リスト）を与えられても、答えの内容が毎回会話の流れとしては同じものしか出せない。</li>
</ul>
</li>
<li> HRED は短く情報量に乏しい答えをしがちである。

<ul>
<li> 同じコンテキスト（発話リスト）を与えられても、それに続く発話は全く異なるものでありうる

<ul>
<li> 「おはよう」「やあおはよう」 というコンテキストに対し 「いい天気だね」 も 「昨日の件どうなった？」も会話として成立する</li>
</ul>
</li>
<li> これらを決定論的に学習しようとすると、結果「無難な」答えつまり短いよくある答えを学ぶ傾向がある。

<ul>
<li> 「うん」「そうだね」「・・・」など。</li>
</ul>
</li>
</ul>
</li>
</ul>

<p>これに対し、 VHRED では Context RNN の部分に確率的なノイズを与えて学習することで上記の問題を解決します。</p>

<ul>
<li>VHRED は会話の流れを表す Context RNN にノイズを乗せることで、同じコンテキストに対しても字面だけではない多様な返答ができる</li>
<li>VHRED はコンテキストに対する返答のばらつきを Context RNN の確率的な幅で吸収することでそれらをうまく学習できる</li>
</ul>

<p>特に論文では VHRED では HRED などの従来の会話モデルに比べより長い文章を生成する傾向があることが書かれています。</p>

<h2>
<span id="vhredの数式的な理解" class="fragment"></span><a href="#vhred%E3%81%AE%E6%95%B0%E5%BC%8F%E7%9A%84%E3%81%AA%E7%90%86%E8%A7%A3"><i class="fa fa-link"></i></a>VHREDの数式的な理解</h2>

<p>VHRED は　HRED に VAE の潜在変数の概念を追加したものとみなせますが、 HRED 側から入るよりも VAE 側から数式的に理解していくほうが近道です。論文の数式を VAE と比較しながら読み解いていきます。</p>

<p>VHRED は、 $w_i$ で表される、 i 番目の発話が i=1, ..., n-1 まで並んだ状態での、 $w_n$ の発話について考える問題となっています。<br>
ここで n は現在の発話の数で、一つの会話全体で N 個の発話があるとします。<br>
各発話 $w_i$ は各単語等トークン $w_{i,1}, ..., w_{i,m}$ から成っているとします。</p>

<p>例えば、「おなかが減った」「そろそろ行く？」「ラーメンがいいな」という文章であれば</p>

<ul>
<li>$w_1$: おなかが減った

<ul>
<li>$w_{1,1}$: おなか</li>
<li>$w_{1,2}$: が</li>
<li>...</li>
</ul>
</li>
<li>$w_2$: そろそろ行く？</li>
<li>...</li>
</ul>

<p>などとなります。</p>

<p>VHRED では $logP_\theta(w_1, ..., w_N)$ を観測された w のセットに対し最大化しようとします。</p>

<h3>
<span id="潜在変数-z-の分布" class="fragment"></span><a href="#%E6%BD%9C%E5%9C%A8%E5%A4%89%E6%95%B0-z-%E3%81%AE%E5%88%86%E5%B8%83"><i class="fa fa-link"></i></a>潜在変数 z の分布</h3>

<p>VAE では z は標準正規分布 $\mathscr{N}(0, I)$ に従いますが、 VHRED では $z_n$ 以下の $\mu_{prior}, \sigma_{prior}$ という関数によって平均と分散が決まる正規分布に従うとされます。<br>
（付録1の VAE の式変形上は Q は必ずしも標準正規分布である必要は無いですね。）</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>P_\theta(z_n|w_1, ..., w_{n-1}) = \mathscr{N}(\mu_{prior}(w_1, ..., w_{n-1}), \sigma_{prior}(w_1, ..., w_{n-1}))\\
</code></pre></div></div>

<p>添字が n-1 までで n は含まれないのがキーポイントになってきます。</p>

<h3>
<span id="尤度-logp-を最大化する" class="fragment"></span><a href="#%E5%B0%A4%E5%BA%A6-logp-%E3%82%92%E6%9C%80%E5%A4%A7%E5%8C%96%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>尤度 logP を最大化する</h3>

<p>VAE と同じく Lower Bound が求められそれを最大化します。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(w_1, ..., w_N) \geqq \sum_{n=1}^N\left\{ -KL[Q_\phi(z_n|w_1, ..., w_n)||P_\theta(z_n|w_1, ..., w_{n-1})] + E_{Q_\phi(z_n|w_1, ..., w_n)}[logP_\theta(w_n|z_n, w_1, ..., w_{n-1})] \right\}
</code></pre></div></div>

<p>この式を注意深く見てみると、添字が $n$ の部分と $n-1$ の部分が入り混じっています。実はこの数式の $w_{n-1}$ と $w_n$ の間にはとても大きな溝があります。<br>
VAE での $logP_\theta$ は</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(x) \geqq -KL[Q_\phi(z|x)||P_\theta(z)] + E_{Q_\phi(z|x)}[logP_\theta(x|z)]
</code></pre></div></div>

<p>でしたが、ここで</p>

<ul>
<li>$z$ -&gt; $z_n$</li>
<li>$x$ -&gt; $w_n$</li>
</ul>

<p>として、各確率に条件 $|w_1, ..., w_{n-1}$ をつけてnを1~Nまで和をとると VHRED の式になることがわかります。（<a href="https://qiita.com/halhorn/private/646d323ac457715866d4#%E4%BB%98%E9%8C%B22-vhred-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA">付録2</a>）<br>
VHRED の式で $w_n$ を $x$ としてみるとわかりやすいかもしれません。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(w_1, ..., w_N) \geqq \sum_{n=1}^N\left\{ -KL[Q_\phi(z_n|x, w_1, ...,w_{n-1})||P_\theta(z_n|w_1, ..., w_{n-1})] + E_{Q_\phi(z_n|x, w_1, ..., w_{n-1})}[logP_\theta(x|z_n, w_1, ..., w_{n-1})] \right\}
</code></pre></div></div>

<p>つまり VHRED は数式的には、</p>

<ul>
<li>$w_1, ..., w_{n-1}$ が事前に与えられている状態での</li>
<li>$w_n$ と $z_n$ での VAE</li>
</ul>

<p>と見ることができます。<br>
VAE では $Q_\phi(z|x)$ は $\mathscr{N}(\mu(x), \sigma(x))$ であるとされましたが、 VHRED では $\mathscr{N}(\mu_{posterior}(w_1, ..., w_n), \sigma_{posterior}(w_1, ..., w_n))$ とします。<br>
（上に出てきた <code>prior</code> の方は $w_1, ..., w_{n-1}$ のみなのに対して、この <code>posterior</code> は $w_n$ も入っていますね。）</p>

<p>VAE で $Q_\phi(z|x)$ の $\mu, \sigma$ を $P_\theta(z) = \mathscr{N}(0,I)$ つまり0, I に近づけたように、 VHRED では $\mu_{posterior}, \sigma_{posterior}$ を $\mu_{prior}, \sigma_{prior}$ に近づけるよう学習を行います。</p>

<p>これを VAE 風の図で書いてみるとこのようになります。<br>
<a href="https://camo.qiitausercontent.com/7f95b43f79b81b6b6d07209728e7a829f551597b/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f37656133363064652d643666392d663433382d633938332d3234623336323136653264632e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F7ea360de-d6f9-f438-c983-24b36216e2dc.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=0bec207c9b42f1b6844ccef6cf205c87" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/7ea360de-d6f9-f438-c983-24b36216e2dc.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F7ea360de-d6f9-f438-c983-24b36216e2dc.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=d59da54338811a135914080d4ea9cff1 1x" loading="lazy"></a></p>

<h3>
<span id="vhred-がやろうとしていること数式的な方向から" class="fragment"></span><a href="#vhred-%E3%81%8C%E3%82%84%E3%82%8D%E3%81%86%E3%81%A8%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E3%81%93%E3%81%A8%E6%95%B0%E5%BC%8F%E7%9A%84%E3%81%AA%E6%96%B9%E5%90%91%E3%81%8B%E3%82%89"><i class="fa fa-link"></i></a>VHRED がやろうとしていること：数式的な方向から</h3>

<p>生成も学習も、 $w_1, ..., w_{n-1}$ によって条件付けされた状態での潜在変数 $z_n$ の学習と、その $z_n$ からの $w_n$ つまり発話生成を行っていると考えられます。</p>

<ul>
<li>学習

<ul>
<li>$w_1, ..., w_{n-1}$ によって条件付けされた状態で</li>
<li>$w_n$ を入力として（潜在変数 $z_n$ を介して） $w_n$ を出力する学習</li>
<li>$z_n$ も $w_1, ..., w_{n-1}$ によって条件付けされたある正規分布に従うよう正則化がされる</li>
</ul>
</li>
<li>生成

<ul>
<li>$w_1, ..., w_{n-1}$ によって条件付けされた状態で</li>
<li>$z_n$ をサンプリングし</li>
<li>（VAE の文脈での）Decoder で $w_n$ を生成する</li>
</ul>
</li>
</ul>

<h2>
<span id="vhred-のニューラルネット的な理解" class="fragment"></span><a href="#vhred-%E3%81%AE%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E7%9A%84%E3%81%AA%E7%90%86%E8%A7%A3"><i class="fa fa-link"></i></a>VHRED のニューラルネット的な理解</h2>

<p><a href="https://camo.qiitausercontent.com/f41ed0745eb83b0278272a70f3c8d89370cd3abe/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f32663132633863332d323837662d313535332d383737612d3337366437613466336666302e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F2f12c8c3-287f-1553-877a-376d7a4f3ff0.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=cda12235bf193815c9508472b3cd01d9" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/2f12c8c3-287f-1553-877a-376d7a4f3ff0.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F2f12c8c3-287f-1553-877a-376d7a4f3ff0.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=e14ab6ebb7072510980b2baaf9e63003 1x" loading="lazy"></a><br>
再び VHRED のモデル図です。<br>
先程の数式的な方面からの理解により、なぜ学習時に <code>posterior parametarization</code> つまり今生成しようとしている次の発話 $w_n$ が入力になってるんだ？とかが理解できると思います。</p>

<p>先程の数式とネットワークが以下のように対応します</p>

<ul>
<li>$|w_1, ..., w_{n-1}$ の条件： n-1 までのデータを Encoder RNN と Context RNN に入力したときの Context RNN の final state</li>
<li>$w_n$:

<ul>
<li>エンコード時： n のデータでの Encoder RNN</li>
<li>デコード時： Decoder RNN の出力</li>
</ul>
</li>
<li>$Q_\phi(z_n|w_n, w_1, ..., w_{n-1})$: Context RNN と Encoder RNN の final state を FNN に食わせたもの

<ul>
<li>出力は $\mu_{posterior}, \sigma_{posterior}$</li>
</ul>
</li>
<li>$P_\theta(w_n|z_n, w_1, ..., w_{n-1})$: Decoder RNN

<ul>
<li>入力

<ul>
<li>$\mu_{posterior}, \sigma_{posterior}$ からサンプリングされた z</li>
<li>Context RNN の final state</li>
</ul>
</li>
</ul>
</li>
</ul>

<h3>
<span id="github-のコードでのネットワーク" class="fragment"></span><a href="#github-%E3%81%AE%E3%82%B3%E3%83%BC%E3%83%89%E3%81%A7%E3%81%AE%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF"><i class="fa fa-link"></i></a>Github のコードでのネットワーク</h3>

<p><a href="https://github.com/julianser/hed-dlg-truncated/blob/master/dialog_encdec.py#L1723" rel="nofollow noopener" target="_blank">本家の Github に上がっているコード</a>をざっと見た感じ以下のようなネットワークになっているようです。<br>
<a href="https://camo.qiitausercontent.com/6c701c2792ddd4ce3a10081e40644754346b46c9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f65613862333866362d386665622d373663392d333531392d6334636364376134316239662e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fea8b38f6-8feb-76c9-3519-c4ccd7a41b9f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=22ea71a360d85c7291e8c5391fb43d33" alt="image.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/61079/ea8b38f6-8feb-76c9-3519-c4ccd7a41b9f.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fea8b38f6-8feb-76c9-3519-c4ccd7a41b9f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=b20b0582fbbabcf77dff9dc3a1288ad4 1x" loading="lazy"></a></p>

<ul>
<li>ボックス： 処理とその返り値の変数名

<ul>
<li>一行目： 返り値の変数名</li>
<li>二行目： その処理をおこなう関数など</li>
</ul>
</li>
<li>太い茶色枠は HRED にもある要素</li>
<li>プログラムは設定によって様々な構成の NN を作れるよう書かれているので、図では図右上の条件でのものを書いています</li>
<li>一部省略した部分などもあります</li>
</ul>

<h1>
<span id="おわりに" class="fragment"></span><a href="#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB"><i class="fa fa-link"></i></a>おわりに</h1>

<p>最初は VHRED 論文の自身での整理とチームメンバーへの共有の目的で書き始めたこの記事ですが、途中から Advent Calendar に出そうという気持ちになり、 Deep Learning の対話モデルの話にしようと風呂敷を広げ続けた結果、このような長大な文章になってしまったことをお詫びいたします。<br>
DeepLearning といえば画像！ということで画像・CNN から Deep Learning に入った方は多いと思いますが、自然言語処理、とくに言語の生成に興味を持ってくれる Deep Learner が増えたらいいなと思っています。</p>

<p>私自身大学時代にロボットの視覚と運動指令を結びつけるモデルとして RNN は使っていたものの、今のチームに移動してから初めて自然言語処理で Deep Learning をはじめた人です。<br>
ロボットを動かすのにせよ、画像を生成するのにせよ、自然言語を生成するのにせよ、何かを生成できるモデルというのはとてもおもしろいです。何か生き物のようなものを感じます。<br>
この面白さに触れられる人が増えますように。</p>

<h1>
<span id="付録" class="fragment"></span><a href="#%E4%BB%98%E9%8C%B2"><i class="fa fa-link"></i></a>付録</h1>

<h2>
<span id="付録0-よく使う式変形定義" class="fragment"></span><a href="#%E4%BB%98%E9%8C%B20-%E3%82%88%E3%81%8F%E4%BD%BF%E3%81%86%E5%BC%8F%E5%A4%89%E5%BD%A2%E5%AE%9A%E7%BE%A9"><i class="fa fa-link"></i></a>付録0. よく使う式変形・定義</h2>

<h3>
<span id="a-確率の積分" class="fragment"></span><a href="#a-%E7%A2%BA%E7%8E%87%E3%81%AE%E7%A9%8D%E5%88%86"><i class="fa fa-link"></i></a>a. 確率の積分</h3>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>\int_zP(z)dz = 1
</code></pre></div></div>

<h3>
<span id="b-期待値" class="fragment"></span><a href="#b-%E6%9C%9F%E5%BE%85%E5%80%A4"><i class="fa fa-link"></i></a>b. 期待値</h3>

<p>確率密度関数 $Q(z)$ に対し値 $X(z)$ の期待値</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>E_{Q(z)}[X(z)] = \int_zX(z)Q(z)dz
</code></pre></div></div>

<h3>
<span id="c-ベイズの定理" class="fragment"></span><a href="#c-%E3%83%99%E3%82%A4%E3%82%BA%E3%81%AE%E5%AE%9A%E7%90%86"><i class="fa fa-link"></i></a>c. ベイズの定理</h3>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>P(x,z) = P(z|x)P(x)
</code></pre></div></div>

<h2>
<span id="d-kullback-leibler-divergence" class="fragment"></span><a href="#d-kullback-leibler-divergence"><i class="fa fa-link"></i></a>d. Kullback-Leibler divergence</h2>

<p><a href="https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%AB%E3%83%90%E3%83%83%E3%82%AF%E3%83%BB%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%BC%E6%83%85%E5%A0%B1%E9%87%8F" rel="nofollow noopener" target="_blank">カルバックライブラー情報量</a>。２つの分布 P, Q の差。非負。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>KL[P(z)||Q(z)] = \int_z P(z)log\left\{\frac{P(z)}{Q(z)}\right\}dz
</code></pre></div></div>

<h2>
<span id="付録1-vae-lower-bound-の導出" class="fragment"></span><a href="#%E4%BB%98%E9%8C%B21-vae-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA"><i class="fa fa-link"></i></a>付録1. VAE Lower Bound の導出</h2>

<p>$logP_\theta(x)$ に対し任意の分布 Q を組み込んで KL の式が出るように変形していくことで下限（Lower Bound）を求めます。<br>
なぜそんな式変形を・・となりますがパターンです。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(x) = \int_zQ_\phi(z|x)logP(x)dz
 ~~ \verb|←a. より追加された部分は1|\\

= \int_zQ_\phi(z|x)log\left\{\frac{P_\theta(x,z)}{P_\theta(z|x)}\right\}dz
 ~~ \verb|←c. ベイズの定理|\\

= \int_zQ_\phi(z|x)log\left\{\frac{Q_\phi(z|x)}{P_\theta(z|x)}\frac{P_\theta(x,z)}{Q_\phi(z|x)}\right\}dz
~~ \verb|←分子分母にQをかけた|\\

= \int_zQ_\phi(z|x)log\left\{\frac{Q_\phi(z|x)}{P_\theta(z|x)}\right\}dz
+ \int_zQ_\phi(z|x)log\left\{\frac{P_\theta(x,z)}{Q_\phi(z|x)}\right\}dz\\

= KL[Q_\phi(z|x)||P_\theta(z|x)] + \int_zQ_\phi(z|x)log\left\{\frac{P_\theta(x,z)}{Q_\phi(z|x)}\right\}dz
~~ \verb|↑d. 左辺は KL|\\

\geqq \int_zQ_\phi(z|x)log\left\{\frac{P_\theta(x,z)}{Q_\phi(z|x)}\right\}dz
~~ \verb|← KL は非負（≧0）|
</code></pre></div></div>

<p>まとめると、この右辺を $L_b$ として以下の不等式が成り立ちます。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(x) \geqq L_b \\
\left(L_b = \int_zQ_\phi(z|x)log\left\{\frac{P_\theta(x,z)}{Q_\phi(z|x)}\right\}dz \right)
</code></pre></div></div>

<p>ちなみに上に出てきた $KL[Q_\phi(z|x)||P_\theta(z|x)]$ は、事後確率 $Q_\phi(z|x), P_\theta(z|x)$ が近いほど0に近づきます。<br>
$L_b$ を大きくしようとすると上記の KL は小さくなるので、 Q は P に近づいていきます。すなわち $Q_\phi(z|x)$ は $P_\theta(z|x)$ の近似とみなせます。</p>

<p>次にこの $L_b$ が KL と期待値となることを示します。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>L_b = \int_zQ_\phi(z|x)log\left\{\frac{P_\theta(x,z)}{Q_\phi(z|x)}\right\}dz \\
= \int_zQ_\phi(z|x)log\left\{\frac{P_\theta(z)}{Q_\phi(z|x)}P_\theta(x|z)\right\}dz
~~ \verb|← c. ベイズの定理|\\

= - \int_zQ_\phi(z|x)log\left\{\frac{Q_\phi(z|x)}{P_\theta(z)}\right\}dz
+ \int_zQ_\phi(z|x)logP_\theta(x|z)dz\\

= -KL[Q_\phi(z|x)||P_\theta(z)] + E_{Q_\phi(z|x)}[logP_\theta(x|z)]
~~ \verb|←d. KL と b. 期待値|

</code></pre></div></div>

<p>これらをまとめて、</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(x) \geqq -KL[Q_\phi(z|x)||P_\theta(z)] + E_{Q_\phi(z|x)}[logP_\theta(x|z)]
</code></pre></div></div>

<h2>
<span id="付録2-vhred-lower-bound-の導出" class="fragment"></span><a href="#%E4%BB%98%E9%8C%B22-vhred-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA"><i class="fa fa-link"></i></a>付録2. VHRED Lower Bound の導出</h2>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(w_1, ..., w_N) = \sum_{n=1}^NlogP_\theta(w_n|w_1, ..., w_{n-1})
</code></pre></div></div>

<p>あとは</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><code>logP_\theta(w_n|w_1, ..., w_{n-1})
</code></pre></div></div>

<p>に対して、 VAE の式を</p>

<ul>
<li>$z$ -&gt; $z_n$</li>
<li>$x$ -&gt; $w_n$</li>
</ul>

<p>として、各確率に条件 $|w_1, ..., w_{n-1}$ をつけて全く同じ計算をするだけです。</p>
</div></div></section><div class="style-r2q46l"><button aria-label="X(Twitter)でシェア" title="ポスト" class="style-26pnip" type="button"><svg class="style-1b1cd5z" viewBox="0 0 20 20"><path d="m11.68 8.62 6.55-7.62h-1.55l-5.69 6.62-4.55-6.62h-5.25l6.88 10.01-6.88 7.99h1.55l6.01-6.99 4.8 6.99h5.24l-7.13-10.38zm-2.13 2.47-.7-1-5.54-7.92h2.39l4.47 6.4.7 1 5.82 8.32h-2.39l-4.75-6.79z"></path></svg></button><button aria-label="facebookでシェア" title="Facebookでシェア" class="style-26pnip" type="button"><svg class="style-157xj1g" height="20" viewBox="0 0 20 20" width="20"><path d="m20 10c0-5.52-4.48-10-10-10s-10 4.48-10 10c0 4.99 3.66 9.13 8.44 9.88v-6.99h-2.54v-2.89h2.54v-2.2c0-2.51 1.49-3.89 3.78-3.89 1.09 0 2.24.2 2.24.2v2.46h-1.26c-1.24 0-1.63.77-1.63 1.56v1.88h2.77l-.44 2.89h-2.33v6.99c4.78-.75 8.44-4.89 8.44-9.88z"></path><path d="m13.89 12.89.44-2.89h-2.77v-1.88c0-.79.39-1.56 1.63-1.56h1.26v-2.46s-1.14-.2-2.24-.2c-2.28 0-3.78 1.38-3.78 3.89v2.2h-2.54v2.89h2.54v6.99c.51.08 1.03.12 1.56.12s1.05-.04 1.56-.12v-6.99h2.33z" fill="#ffffff"></path></svg></button><a aria-label="はてなブックマーク" class="style-s47yiq" href="https://b.hatena.ne.jp/entry/s/qiita.com/halhorn/items/646d323ac457715866d4" rel="noopener noreferrer" target="_blank" title="はてなブックマーク"><svg class="style-1uu7u3w" viewBox="0 0 20 20"><rect height="18" rx="3.67" width="18" x="1" y="1" fill="#1d9bf0"></rect><g fill="#ffffff"><path d="m11.01 10.29c-.33-.36-.78-.57-1.36-.61.52-.14.89-.35 1.13-.62s.35-.64.35-1.11c0-.37-.08-.69-.24-.97s-.39-.5-.69-.67c-.26-.15-.58-.25-.94-.31-.37-.06-1.01-.09-1.93-.09h-2.24v8.18h2.31c.93 0 1.6-.03 2.01-.09s.75-.17 1.03-.32c.34-.18.61-.44.79-.77s.28-.72.28-1.15c0-.61-.16-1.09-.49-1.46zm-3.85-2.57h.48c.55 0 .93.06 1.12.19s.29.34.29.65-.1.5-.31.63c-.21.12-.58.18-1.13.18h-.45v-1.64zm1.9 4.69c-.22.13-.59.2-1.12.2h-.78v-1.79h.81c.54 0 .91.07 1.11.2.2.14.3.38.3.72 0 .31-.11.53-.33.66z"></path><path d="m13.87 12.02c-.57 0-1.04.46-1.04 1.04s.46 1.04 1.04 1.04 1.04-.46 1.04-1.04-.46-1.04-1.04-1.04z"></path><path d="m12.97 5.91h1.8v5.45h-1.8z"></path></g></svg></a></div></div><div><div class="style-itrjxe"></div></div></div></div><div class="style-6rai7j"><div class="style-5jpx49"><div class="style-10h2231"><div class="style-1duml6f"><svg width="64" height="64" viewBox="0 0 64 64" class="style-zq6eqh"><circle cx="32" cy="32" r="19" class="circle"></circle><path fill-rule="evenodd" clip-rule="evenodd" d="M30.5865 41.1378C31.3272 41.8221 32.6722 41.8319 33.4129 41.1574L33.4718 41.1037C38.5554 36.4713 41.8819 33.4401 41.746 29.6619C41.6875 28.0001 40.8396 26.4068 39.4654 25.4684C37.3743 24.0305 34.8841 24.4462 33.1038 25.7333C32.6936 26.0299 32.321 26.3728 31.9997 26.75C31.6783 26.3727 31.3056 26.03 30.8952 25.7338C29.115 24.4491 26.625 24.0385 24.5341 25.4684C23.1599 26.4068 22.312 28.0001 22.2535 29.6619C22.1268 33.4546 25.4697 36.4947 30.5865 41.1378ZM23.7526 29.7133C23.7953 28.5111 24.4144 27.3665 25.38 26.7071L25.3808 26.7065C27.1492 25.4973 29.4926 26.12 30.8579 27.7227L31.543 28.527C31.7826 28.8082 32.2169 28.8082 32.4565 28.527L33.1416 27.7227C34.5102 26.1161 36.8515 25.4914 38.6155 26.7044L38.6195 26.7071C39.5855 27.3667 40.2046 28.512 40.2469 29.7146L40.247 29.7158C40.2969 31.1036 39.73 32.4647 38.3804 34.1435C37.0091 35.8492 34.9938 37.6874 32.403 40.0483C32.3636 40.0842 32.2314 40.1587 32.0103 40.1572C31.7898 40.1556 31.6512 40.0792 31.6044 40.036L31.5995 40.0315L31.5945 40.027C29.0048 37.677 26.9919 35.8432 25.6225 34.1408C24.2752 32.4657 23.7066 31.1034 23.7526 29.7133Z" class="heart-stroke"></path><path d="M33.4129 41.1574C32.6722 41.8319 31.3272 41.8221 30.5865 41.1378C25.4697 36.4947 22.1268 33.4546 22.2535 29.6619C22.312 28.0001 23.1599 26.4068 24.5341 25.4684C27.1071 23.7089 30.2844 24.7363 31.9997 26.75C33.7151 24.7363 36.8924 23.6991 39.4654 25.4684C40.8396 26.4068 41.6875 28.0001 41.746 29.6619C41.8824 33.4546 38.5297 36.4947 33.4129 41.1574Z" class="heart-fill"></path><g class="particles" style="--rotate:30deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g><g class="particles" style="--rotate:102deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g><g class="particles" style="--rotate:174deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g><g class="particles" style="--rotate:246deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g><g class="particles" style="--rotate:318deg"><circle cx="29" cy="19" r="3" fill="var(--color-yellow60)" class="particle" style="--x:26;--y:7;--duration:150ms"></circle><circle cx="35" cy="23" r="2" fill="var(--color-blue60)" class="particle" style="--x:37;--y:10;--duration:200ms"></circle></g></svg><button aria-label="いいね" title="いいね" class="style-10ttvi6"></button></div><a href="/halhorn/items/646d323ac457715866d4/likers" aria-label="156いいね" class="style-l8qm67">156</a></div><div class="style-92283"><button aria-label="ストック" title="ストック" class="style-1xwfn2v"><svg class="style-100tu0r" height="32" viewBox="0 0 32 32" width="32"><path d="M6.5 5H25.5C25.7761 5 26 5.22386 26 5.5V6.5C26 6.77614 25.7761 7 25.5 7H6.5C6.22386 7 6 6.77614 6 6.5V5.5C6 5.22386 6.22386 5 6.5 5ZM24 19V11.5C24 11.3674 23.9473 11.2402 23.8536 11.1464C23.7598 11.0527 23.6326 11 23.5 11H8.5C8.36739 11 8.24021 11.0527 8.14645 11.1464C8.05268 11.2402 8 11.3674 8 11.5V19C8 20.8565 8.7375 22.637 10.0503 23.9497C11.363 25.2625 13.1435 26 15 26H17C18.8565 26 20.637 25.2625 21.9497 23.9497C23.2625 22.637 24 20.8565 24 19ZM6.58579 9.58579C6.96086 9.21071 7.46957 9 8 9H24C24.5304 9 25.0391 9.21071 25.4142 9.58579C25.7893 9.96086 26 10.4696 26 11V19C26 21.3869 25.0518 23.6761 23.364 25.364C21.6761 27.0518 19.3869 28 17 28H15C12.6131 28 10.3239 27.0518 8.63604 25.364C6.94821 23.6761 6 21.3869 6 19V11C6 10.4696 6.21071 9.96086 6.58579 9.58579Z"></path></svg></button><span class="style-n5z2fc">136</span></div><div class="style-79elbk"><button type="button" aria-label="オプションを開く" class="style-rvwogb"><span class="material-symbols-outlined style-1j8uoc8" aria-hidden="true">more_horiz</span></button></div></div></div></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="PersonalArticlePage" data-dom-id="PersonalArticlePage-react-component-b059084f-b34b-46ef-be87-c9b112fb6596">{"authorAnalyticsTrackingId":null,"findyLogoImageUrl":"//cdn.qiita.com/assets/public/popup/logo-findy-bb852343bec19c0eb60a1e3c033a4e78.png","headerBanner":{"bannerDisplayKey":"header_text_banner_displayed","isDisplayable":true},"organizationAnalyticsTrackingId":null,"promptLoginMessageImageSrc":"//cdn.qiita.com/assets/public/image-qiitan_for_login_modal-014e085d3e40a240e3fe8d61b70b29a9.png","isPostCompletePromptShow":false,"postCompleteModalImages":[],"adventCalendarBeingHeldImageSrc":"//cdn.qiita.com/assets/public/advent_calendar/image-advent_calendar_being_held-bafa166cb2e60037f5e4ba3599642d8f.png"}</script>
      
<script src="//d-cache.microad.jp/js/td_qt_access.js" type="text/javascript"></script><script>microadTd.QT.start({"article_category": "NLP,DeepLearning,VHRED"})</script><script>(function(d,u){var b=d.getElementsByTagName("script")[0],j=d.createElement("script");j.async=true;j.src=u;b.parentNode.insertBefore(j,b);})(document,"//img.ak.impact-ad.jp/ut/ff9a3577423c8ed5_4330.js");</script><noscript><iframe frameborder="0" height="0" src="//nspt.unitag.jp/ff9a3577423c8ed5_4330.php" width="0"></iframe></noscript><script async="" src="https://www.googletagmanager.com/gtag/js?id=AW-878053044"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'AW-878053044');</script><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?
n.callMethod.apply(n,arguments):n.queue.push(arguments)};
if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
n.queue=[];t=b.createElement(e);t.async=!0;
t.src=v;s=b.getElementsByTagName(e)[0];
s.parentNode.insertBefore(t,s)}(window, document,'script',
'https://connect.facebook.net/en_US/fbevents.js');
fbq('init', '1792588824374455');
fbq('track', 'PageView');</script><noscript><img height="1" src="https://www.facebook.com/tr?id=1792588824374455&amp;ev=PageView&amp;noscript=1" style="display:none" width="1" /></noscript></div><footer id="globalfooter" class="st-footer"><div class="st-footer_container"><div class="st-footer_start"><div class="st-footer_signature"><a class="st-footer_logo" aria-label="トップに戻る" href="/"><svg alt="" role="presentation" viewbox="0 0 426.57 130" xmlns="http://www.w3.org/2000/svg"><circle cx="167.08" cy="21.4" r="12.28" /><path d="M250.81 29.66h23.48v18.9h-23.48z" /><path d="M300.76 105.26a22.23 22.23 0 01-6.26-.86 12.68 12.68 0 01-5.17-3 14.41 14.41 0 01-3.56-5.76 28 28 0 01-1.3-9.22V48.56h29.61v-18.9h-29.52V3.29h-20.17v83.34q0 11.16 2.83 18.27a27.71 27.71 0 007.7 11.2 26.86 26.86 0 0011.43 5.62 47.56 47.56 0 0012.34 1.53h15.16v-18zM0 61.7a58.6 58.6 0 015-24.21A62.26 62.26 0 0118.73 17.9 63.72 63.72 0 0139 4.78 64.93 64.93 0 0164 0a65 65 0 0124.85 4.78 64.24 64.24 0 0120.38 13.12A62 62 0 01123 37.49a58.6 58.6 0 015 24.21 58.34 58.34 0 01-4 21.46 62.8 62.8 0 01-10.91 18.16l11.1 11.1a10.3 10.3 0 010 14.52 10.29 10.29 0 01-14.64 0l-12.22-12.41a65 65 0 01-15.78 6.65 66.32 66.32 0 01-17.55 2.3 64.63 64.63 0 01-45.23-18A62.82 62.82 0 015 85.81 58.3 58.3 0 010 61.7zm21.64.08a43.13 43.13 0 0012.42 30.63 42.23 42.23 0 0013.43 9.09A41.31 41.31 0 0064 104.8a42 42 0 0030-12.39 42.37 42.37 0 009-13.64 43.43 43.43 0 003.3-17 43.77 43.77 0 00-3.3-17A41.7 41.7 0 0080.55 22 41.78 41.78 0 0064 18.68 41.31 41.31 0 0047.49 22a42.37 42.37 0 00-13.43 9.08 43.37 43.37 0 00-12.42 30.7zM331.89 78a47.59 47.59 0 013.3-17.73 43.22 43.22 0 019.34-14.47A44.25 44.25 0 01359 36a47.82 47.82 0 0118.81-3.58 42.72 42.72 0 019.26 1 46.5 46.5 0 018.22 2.58 40 40 0 017 3.84 44.39 44.39 0 015.71 4.63l1.22-9.47h17.35v85.83h-17.35l-1.17-9.42a42.54 42.54 0 01-5.84 4.67 43.11 43.11 0 01-7 3.79 44.86 44.86 0 01-8.17 2.59 43 43 0 01-9.22 1A47.94 47.94 0 01359 119.9a43.3 43.3 0 01-14.47-9.71 44.17 44.17 0 01-9.34-14.47 47 47 0 01-3.3-17.72zm20.27-.08a29.16 29.16 0 002.17 11.34 27 27 0 005.92 8.88 26.69 26.69 0 008.76 5.76 29.19 29.19 0 0021.44 0 26.11 26.11 0 008.72-5.76 27.57 27.57 0 005.88-8.84 29 29 0 002.16-11.38 28.62 28.62 0 00-2.16-11.22 26.57 26.57 0 00-5.93-8.8 27.68 27.68 0 00-19.51-7.9 28.29 28.29 0 00-10.77 2.05 26.19 26.19 0 00-8.71 5.75 27.08 27.08 0 00-5.84 8.8 28.94 28.94 0 00-2.13 11.31zm-194.97-30.5h19.78v73.54h-19.78zm49.25 0h19.78v73.54h-19.78z" /><circle cx="216.33" cy="21.4" r="12.28" /></svg></a><div class="st-footer_catchcopy">How developers code is here.</div></div><small class="st-footer_copyright">© 2011-2024<span class="ml-1of2" translate="no">Qiita Inc.</span></small></div><nav class="st-footer_navigation"><dl class="st-footer_menu"><dt class="st-footer_menuTitle">ガイドとヘルプ</dt><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/about">About</a></dd><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/terms">利用規約</a></dd><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/privacy">プライバシーポリシー</a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="http://help.qiita.com/ja/articles/qiita-community-guideline">ガイドライン</a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="https://help.qiita.com/ja/articles/others-brand-guideline">デザインガイドライン</a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="https://github.com/increments/qiita-discussions/discussions/116">ご意見</a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="https://help.qiita.com">ヘルプ</a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="https://business.qiita.com/?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">広告掲載</a></dd></dl><dl class="st-footer_menu"><dt class="st-footer_menuTitle">コンテンツ</dt><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/release-notes">リリースノート</a></dd><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/official-events">公式イベント</a></dd><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/official-columns">公式コラム</a></dd><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/opportunities">募集</a></dd><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/advent-calendar/2023">アドベントカレンダー</a></dd><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/qiita-award">Qiita 表彰プログラム</a></dd><dd class="st-footer_menuItem"><a class="st-footer_menuLink" href="/api/v2/docs">API</a></dd></dl><dl class="st-footer_menu"><dt class="st-footer_menuTitle">SNS</dt><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="https://twitter.com/qiita"><img alt="X(Twitter)" width="14" height="14" loading="lazy" src="https://cdn.qiita.com/assets/brand_icons/icon-x_white-a2cf94da373d24e4cf6e4848a3a1c4a8.svg" /><span class="ml-1of2">Qiita（キータ）公式</span></a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="https://twitter.com/qiita_milestone"><img alt="X(Twitter)" width="14" height="14" loading="lazy" src="https://cdn.qiita.com/assets/brand_icons/icon-x_white-a2cf94da373d24e4cf6e4848a3a1c4a8.svg" /><span class="ml-1of2">Qiita マイルストーン</span></a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="https://twitter.com/qiitapoi"><img alt="X(Twitter)" width="14" height="14" loading="lazy" src="https://cdn.qiita.com/assets/brand_icons/icon-x_white-a2cf94da373d24e4cf6e4848a3a1c4a8.svg" /><span class="ml-1of2">Qiita 人気の投稿</span></a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noopener noreferrer" class="st-footer_menuLink" href="https://www.facebook.com/qiita/"><img alt="Facebook" width="14" height="14" loading="lazy" src="https://cdn.qiita.com/assets/brand_icons/icon-facebook_white-f010f8b99b612698389aa7d0682d4ebe.svg" /><span class="ml-1of2">Qiita（キータ）公式</span></a></dd></dl><dl class="st-footer_menu"><dt class="st-footer_menuTitle">Qiita 関連サービス</dt><dd class="st-footer_menuItem"><a target="_blank" class="st-footer_menuLink" href="https://teams.qiita.com/">Qiita Team</a></dd><dd class="st-footer_menuItem"><a target="_blank" class="st-footer_menuLink" href="https://jobs.qiita.com?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">Qiita Jobs</a></dd><dd class="st-footer_menuItem"><a target="_blank" class="st-footer_menuLink" href="https://zine.qiita.com?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">Qiita Zine</a></dd><dd class="st-footer_menuItem"><a target="_blank" rel="noreferrer" class="st-footer_menuLink" href="https://suzuri.jp/qiita">Qiita 公式ショップ</a></dd></dl><dl class="st-footer_menu"><dt class="st-footer_menuTitle">運営</dt><dd class="st-footer_menuItem"><a target="_blank" class="st-footer_menuLink" href="https://corp.qiita.com/company/">運営会社</a></dd><dd class="st-footer_menuItem"><a target="_blank" class="st-footer_menuLink" href="https://corp.qiita.com/jobs/">採用情報</a></dd><dd class="st-footer_menuItem"><a target="_blank" class="st-footer_menuLink" href="https://blog.qiita.com">Qiita Blog</a></dd></dl></nav></div></footer><div id="Snackbar-react-component-66902e13-aa8a-4398-aca6-df13070b6edf"></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="Snackbar" data-dom-id="Snackbar-react-component-66902e13-aa8a-4398-aca6-df13070b6edf">{}</script>
      
<div id="LoginModal-react-component-50f5686d-1db3-4b5d-b24f-282aca4dd617"></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="LoginModal" data-dom-id="LoginModal-react-component-50f5686d-1db3-4b5d-b24f-282aca4dd617">{"imageUrl":"//cdn.qiita.com/assets/public/image-qiitan_for_login_modal_in_reaction-8f8371248606043be50361a0dadb3f31.png","githubAuthUrl":"https://oauth.qiita.com/auth/github?callback_action=login_or_signup\u0026realm=qiita\u0026redirect_to=%2Fhalhorn%2Fitems%2F646d323ac457715866d4","googleAuthUrl":"https://oauth.qiita.com/auth/google?callback_action=login_or_signup\u0026realm=qiita\u0026redirect_to=%2Fhalhorn%2Fitems%2F646d323ac457715866d4","twitterAuthUrl":"https://oauth.qiita.com/auth/twitter?callback_action=login_or_signup\u0026realm=qiita\u0026redirect_to=%2Fhalhorn%2Fitems%2F646d323ac457715866d4"}</script>
      
</div><div id="dataContainer" style="display: none;" data-config="{&quot;actionPath&quot;:&quot;public/items#show&quot;,&quot;settings&quot;:{&quot;analyticsTrackingId&quot;:&quot;G-KEVS5DBRVN&quot;,&quot;tagManagerId&quot;:&quot;GTM-W9W5TX4&quot;,&quot;assetsMap&quot;:{},&quot;csrfToken&quot;:&quot;NbkFZhoSCu1BwoKsgD9yyTDUVogUMO3pdX62CAxfrFSTtXgm8ocFqdqN2FxGa7XhD1yGlsT7arR5Nb5Ahb7_ag&quot;,&quot;locale&quot;:&quot;ja&quot;,&quot;pushOne&quot;:{&quot;accessToken&quot;:&quot;4bc4bb057a0601d388a7b9e499ca2307e9eb4fc172804e310c59e511d7cac356&quot;,&quot;dialogImageUrl&quot;:&quot;//cdn.qiita.com/assets/public/push_notification/image-qiitan-572179a3bbde375850422ea48b2b6272.png&quot;},&quot;textlint&quot;:{&quot;workerUrl&quot;:&quot;//cdn.qiita.com/assets/public/textlint-worker-0829171f0828e44da54b22ec4f2a17e3.min.js&quot;}},&quot;currentUser&quot;:{&quot;defaultStockItemCategories&quot;:[{&quot;encryptedId&quot;:&quot;BAhJIh5TdG9ja0l0ZW1DYXRlZ29yeS0zOTYzOTY0BjoGRVQ=--ad07ff9e51094f260b870e3277347ffb2ec74acdd8d28ee0b805b7394bf5834e&quot;,&quot;isDefault&quot;:true,&quot;isDestroyableByViewer&quot;:false,&quot;isUpdatableByViewer&quot;:false,&quot;name&quot;:&quot;あとで読む&quot;,&quot;stockItemsCount&quot;:0}],&quot;encryptedId&quot;:&quot;BAhJIhNWaWV3ZXItMzczMzQ4NwY6BkVU--c2e1a5ebea5c1ce040510ad22a94d2d6e49de07151cb1e81305464db198503da&quot;,&quot;isBetaReleaseEnabled&quot;:false,&quot;isDarkModeEnabled&quot;:true,&quot;isEarlyAdapter&quot;:false,&quot;isEmailConfirmed&quot;:true,&quot;isEmailRegistered&quot;:true,&quot;isExceededAiUsageLimit&quot;:false,&quot;isFindyRegistered&quot;:false,&quot;isPasswordRegistered&quot;:false,&quot;isJobseeker&quot;:false,&quot;isStaff&quot;:false,&quot;isTeamOnly&quot;:false,&quot;isTwitterLoginOnly&quot;:false,&quot;monthlyPublicImageUploadableSizeLimit&quot;:104857600,&quot;name&quot;:&quot;&quot;,&quot;contribution&quot;:0,&quot;originalId&quot;:3733487,&quot;profileImageUrl&quot;:&quot;https://lh3.googleusercontent.com/a/ACg8ocJVPijPJ1D2gFKMC289nZRHFlTW9EYMMUEZS0MICcdi=s96-c&quot;,&quot;profileImageUrlW48&quot;:&quot;https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh3.googleusercontent.com%2Fa%2FACg8ocJVPijPJ1D2gFKMC289nZRHFlTW9EYMMUEZS0MICcdi%3Ds96-c?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=c696664d711a3e6342e78ee999cd1f2d&quot;,&quot;profileImageUrlW75&quot;:&quot;https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh3.googleusercontent.com%2Fa%2FACg8ocJVPijPJ1D2gFKMC289nZRHFlTW9EYMMUEZS0MICcdi%3Ds96-c?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=75\u0026s=090182d19092504e19ad0f970c09e16e&quot;,&quot;remainingPublicImageUploadableSizeInCurrentMonth&quot;:104857600,&quot;theme&quot;:&quot;SYSTEM&quot;,&quot;unreadNotificationsCount&quot;:0,&quot;urlName&quot;:&quot;tian18351898629&quot;,&quot;webPushNotificationKey&quot;:&quot;034468f00f894e34a4ba&quot;,&quot;organizations&quot;:{&quot;edges&quot;:[],&quot;totalCount&quot;:0}},&quot;additionalParams&quot;:{&quot;query_parameters&quot;:{},&quot;path_parameters&quot;:{&quot;controller&quot;:&quot;public/items&quot;,&quot;action&quot;:&quot;show&quot;,&quot;user_id&quot;:&quot;halhorn&quot;,&quot;type&quot;:&quot;items&quot;,&quot;id&quot;:&quot;646d323ac457715866d4&quot;},&quot;request_id&quot;:&quot;cebc078e-2437-491f-b9ca-cee2f0199af1&quot;,&quot;user_id&quot;:3733487}}" /></body></html><script type="application/json" data-js-react-on-rails-store="AppStoreWithReactOnRails">{"snackbar":{"type":"","body":"","isActive":false},"adventCalendar":{"adventCalendar":{"currentDateTime":"Sun, 03 Mar 2024 19:00:47 +0900","isAdventCalendarBeingHeld":false,"isCalendarCreatable":false,"isLatestHeldYear":true,"isPreRegistering":false,"isRankingBeingHeld":false,"isSubscribable":false,"year":2023,"years":[2023,2022,2021,2020,2019,2018,2017,2016,2015,2014,2013,2012,2011]}},"article":{"article":{"body":"\u003cp\u003eこんにちは。 DeepLearning で対話ロボットを作ろうとしているインコです。 \u003cbr\u003e\nこの記事は mixi Advent Calendar 2017 の 12/03 の記事です。\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"概要\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%A6%82%E8%A6%81\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e概要\u003c/h1\u003e\n\n\u003cp\u003e近年対話モデルとして DeepLearning を用いた End to End のアプローチが盛んに行われています。\u003cbr\u003e\nこの記事ではこれらに用いられるモデルとして一問一答に使われる Seq2Seq から出発して、複数発話コンテキストを扱いベイズ的なアプローチを組み込んだ VHRED を理解することをゴールとします。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/e02332e7f2a0a331670237ff83b2bc2059a3de26/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f64663061336264362d396564302d326665612d333139382d3163353435373563326334652e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fdf0a3bd6-9ed0-2fea-3198-1c54575c2c4e.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=a8a60d3126f604284b0c4569467bfca2\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/df0a3bd6-9ed0-2fea-3198-1c54575c2c4e.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fdf0a3bd6-9ed0-2fea-3198-1c54575c2c4e.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=9435aa20ad2af5bf84078d69c238ef95 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"会話モデルのもろもろ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E4%BC%9A%E8%A9%B1%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%82%82%E3%82%8D%E3%82%82%E3%82%8D\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e会話モデルのもろもろ\u003c/h1\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"seq2seq\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#seq2seq\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eSeq2Seq\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1506.05869.pdf\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://arxiv.org/pdf/1506.05869.pdf\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/587c06a1455d43087135887ca8049399f5b6ba41/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f33626435313138392d663638352d313332382d373730642d6330616464376636646238312e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F3bd51189-f685-1328-770d-c0add7f6db81.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=e59f6549665c251c868634a9656033f2\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/3bd51189-f685-1328-770d-c0add7f6db81.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F3bd51189-f685-1328-770d-c0add7f6db81.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=0b0979faddae658fdd6103bc4d7d3787 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eDeepLearning で対話！と言ったときにまず出てくる基本的なモデルが Sequence to Sequence こと Seq2Seq です。\u003cbr\u003e\nこれは発話・応答のシーケンスのペアを学習させることで、発話から応答を生成するモデルです。\u003cbr\u003e\n\u003ca href=\"https://www.tensorflow.org/tutorials/seq2seq/\" rel=\"nofollow noopener\" target=\"_blank\"\u003etensorflow 上\u003c/a\u003eにも実装があります。\u003c/p\u003e\n\n\u003cp\u003e対話モデル以外にも様々な方面に応用されていますが、特に入力を日本語の文、出力を英語の文などとして翻訳モデルとして盛んに使われています。 Google 翻訳が 2016/11 に劇的に精度向上したのが\u003ca href=\"http://trendy.nikkeibp.co.jp/atcl/column/16/032300106/090700005/\" rel=\"nofollow noopener\" target=\"_blank\"\u003e話題\u003c/a\u003eになりましたが、この\u003ca href=\"https://research.googleblog.com/2017/07/building-your-own-neural-machine.html\" rel=\"nofollow noopener\" target=\"_blank\"\u003e Google 翻訳にも Seq2Seq が使われている\u003c/a\u003eようです。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"構造\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%A7%8B%E9%80%A0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e構造\u003c/h3\u003e\n\n\u003cp\u003eLSTM などの RNN を用いたネットワークで２つの部分から構成されます\u003cbr\u003e\n（LSTM?? RNN?? という方は\u003ca href=\"https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca\" id=\"reference-dec2ed67fcab51f8552b\"\u003eLSTMネットワークの概要\u003c/a\u003e を読むのがおすすめです）\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eEncoder RNN\u003c/strong\u003e: （図の Context） 人間からの問いかけの文章を単語などトークンに区切って渡します\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eDecoder RNN\u003c/strong\u003e: (図の Reply) システムからの応答を単語などトークン毎に生成します\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eEncoder RNN は図のトークン A, B, C を入力として受け取ったあとの final state を Decoder RNN の initial state として渡します。\u003cbr\u003e\nこの Encoder RNN の final state は thought vector と呼ばれており、 A, B, C という文章全体の情報を持つベクトルとなるとされています。\u003c/p\u003e\n\n\u003cp\u003eSeq2Seq をうまく学習させるための研究は盛んに行われており、 LSTM を多層にする、 Encoder RNN を bidirectional RNN にする、 \u003ca href=\"https://qiita.com/halhorn/items/614f8fe1ec7663e04bea\" id=\"reference-7db01c735ce2b9a9dc20\"\u003eAttention Mechanism (日本語解説)\u003c/a\u003e を使うなど様々な性能改善手法があります。\u003c/p\u003e\n\n\u003ch4\u003e\n\u003cspan id=\"もうすこし具体的に\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%82%E3%81%86%E3%81%99%E3%81%93%E3%81%97%E5%85%B7%E4%BD%93%E7%9A%84%E3%81%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eもうすこし具体的に\u003c/h4\u003e\n\n\u003cp\u003eこれだけだと抽象的でわかりづらいのでもう少し具体的なフローを説明します。\u003cbr\u003e\n「インコは可愛いね」 という文を Seq2Seq に入力して、 「可愛いよね」 が生成されるまでの過程は以下のようになります。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/2775c0917ff4e0cca6ef5b96cfb28de50330d6c3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f36653865636338342d346364362d366332372d363662372d6663383065346138306635332e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F6e8ecc84-4cd6-6c27-66b7-fc80e4a80f53.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=4d896cf1d11766bec1716207c2647a89\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/6e8ecc84-4cd6-6c27-66b7-fc80e4a80f53.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F6e8ecc84-4cd6-6c27-66b7-fc80e4a80f53.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=84022675d50e7e1aa19bc5262a984ff7 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eEncoding\n\n\u003col\u003e\n\u003cli\u003e\n\u003cfont color=\"orange\"\u003eTokenize:\u003c/font\u003e 文章を単語等（token と呼びます）毎に分割し、 token 毎の ID に変換します。\u003c/li\u003e\n\u003cli\u003e\n\u003cfont color=\"orange\"\u003eEmbedding:\u003c/font\u003e ID から、その token を表す分散表現ベクトルに変換します。\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://deepage.net/bigdata/machine_learning/2016/09/02/word2vec_power_of_word_vector.html\" rel=\"nofollow noopener\" target=\"_blank\"\u003eWord2Vec\u003c/a\u003e が有名ですね。\u003c/li\u003e\n\u003cli\u003e私のチームではこれを文字単位で行う \u003ca href=\"http://alpha.mixi.co.jp/entry/2017/10/12/154825\" rel=\"nofollow noopener\" target=\"_blank\"\u003eChar2Vec なども試して\u003c/a\u003eいます。\u003c/li\u003e\n\u003cli\u003eword2vec を使わなくとも、 token ID と一対一対応する適当な正規分布からサンプルしたベクトルを入れておけば OK です。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cfont color=\"orange\"\u003eEncoder RNN:\u003c/font\u003e ベクトルを順番に RNN に入力していきます。\n\n\u003cul\u003e\n\u003cli\u003evec1 を RNN に入力して hidden state (横矢印)を出力。この hidden state と次の入力 vec2 をまた RNN に入力してまた hidden state を出力・・を繰り返します。\u003c/li\u003e\n\u003cli\u003e最後の vec4 を入れたときの hidden state を final state としてとっておきます。\u003c/li\u003e\n\u003cli\u003eこの final state が thought vector と呼ばれ、「インコは可愛いね」という文の意味のようなものを表すベクトルとなっています。\u003c/li\u003e\n\u003cli\u003eEncoder とはつまり、「インコ可愛いね」という文（の ID 列）を thought vector にエンコードするものなわけです\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eDecoding\n\n\u003col\u003e\n\u003cli\u003e\n\u003cfont color=\"orange\"\u003eDecoder RNN:\u003c/font\u003e Encoder RNN の final state (thought vector) から、各 token の生成確率を出力していきます\n\n\u003col\u003e\n\u003cli\u003efinal state を Decoder RNN の initial state ととして設定し、特別なシンボル \u003ccode\u003e\u0026lt;GO\u0026gt;\u003c/code\u003e の Embedding を入力\u003c/li\u003e\n\u003cli\u003eRNN の隠れ層に全結合層等を噛まして、 token ID ごとの生成確率を出力。\n\n\u003cul\u003e\n\u003cli\u003e例えば \u003ccode\u003e[0.1, 0.001, 0.3, ..]\u003c/code\u003e なら ID:0 は10%、ID:1は0.1%・・といった具合\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cfont color=\"orange\"\u003eSampling:\u003c/font\u003e 生成確率にもとづいて token をランダムに選びます\n\n\u003cul\u003e\n\u003cli\u003eより精度の良い生成を行うにはここでビームサーチを行います\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cfont color=\"orange\"\u003eEmbedding:\u003c/font\u003e 2で選ばれた token を Embedding して Decoder RNN\nへの次の入力とします。\u003c/li\u003e\n\u003cli\u003e\n\u003cfont color=\"orange\"\u003eDetokenize:\u003c/font\u003e 1-3 を繰り返し、2で得られた token を文字列に直します\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eこのようにして、 Seq2Seq はインコの可愛さに同意することが可能になります。\u003cbr\u003e\nここで最終的に説明したい VHRED への伏線として、　2.2 で次の\u003cstrong\u003etoken (単語等)\u003c/strong\u003eを選ぶときに（重み付き）ランダムサンプリングをしていることを覚えておいて下さい。\u003cbr\u003e\nニューラルネットというとランダム性無く決定論的に生成を行うイメージがありますが、 Seq2Seq ではこのように単語などの並びというレベルでは生成される文にランダム性をもたせることができます。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"できること\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eできること\u003c/h3\u003e\n\n\u003cp\u003e元論文では映画のセリフを学習データとして使うことで以下のように様々な問に答えるモデルができたとしています。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"text\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eHuman: who is skywalker ?\nMachine: he is a hero .\nHuman: who is bill clinton ?\nMachine: he ’s a billionaire .\nHuman: is sky blue or black ?\nMachine: blue .\nHuman: does a cat have a tail ?\nMachine: yes .\nHuman: does a cat have a wing ?\nMachine: no\nHuman: can a cat fly ?\nMachine: no .\n...\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eただし、このモデルは直前の会話のみを Encoder RNN に渡す仕組みですので、それより前の発言から次の発言を生成することはできません。つまり一問一答です。\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"hred\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#hred\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eHRED\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1507.04808.pdf\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://arxiv.org/pdf/1507.04808.pdf\u003c/a\u003e\u003cbr\u003e\n実装： \u003ca href=\"https://github.com/julianser/hed-dlg-truncated\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/julianser/hed-dlg-truncated\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/65bd301a355c4ee9aea826d2492fc82b224be996/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f36383735356131372d616663362d396661372d383062322d6234333234626162333938312e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F68755a17-afc6-9fa7-80b2-b4324bab3981.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=ab14779eb884e8797b1aafe663d769da\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/68755a17-afc6-9fa7-80b2-b4324bab3981.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F68755a17-afc6-9fa7-80b2-b4324bab3981.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=375496f52e403bdd255f9df67eaf9170 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eHierarchical Recurrent Encoder-Decoder の略です。\u003cbr\u003e\nSeq2Seq は一問一答ですが、これを過去の n-1 個の発話から次の n 個目の発話を推測するようにしたのが HRED です。\u003c/p\u003e\n\n\u003cp\u003eSeq2Seq では例えば\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eシステム：「インコ好きだよね？」\u003c/li\u003e\n\u003cli\u003eユーザー：「うん」\u003c/li\u003e\n\u003cli\u003eシステム：＜次の答え＞\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eの次の答えが「うん」のみから生成されるため、おそらくインコに関する話題が次生成されることはありません。\u003cbr\u003e\nHRED では過去 n-1 個の発話から次の発話を生成するため、例えば「インコかわいいよねわかる。」みたいな発話を生成できる可能性があります。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"構造-1\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%A7%8B%E9%80%A0-1\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e構造\u003c/h3\u003e\n\n\u003cp\u003eSeq2Seq は Encoder RNN, Decoder RNN の2段構成でしたが、 HRED は Encoder RNN, Context RNN, Decoder RNN の3段構成です。\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eEncoder RNN: 一つ一つの文章（会話なら過去の一つ一つの発言）をそれを表すベクトルに変換する\u003c/li\u003e\n\u003cli\u003eContext RNN: Encoder のまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する\u003c/li\u003e\n\u003cli\u003eDecoder RNN: Context RNN の情報から応答を生成する\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e2 の Context RNN というレイヤーがあることによって、過去の発話の履歴を加味した返答をできるようになっているということですね。\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"vae\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vae\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVAE\u003c/h2\u003e\n\n\u003cp\u003eVAE は対話モデルではないのですが、最終的に説明をしたい VHRED を数学的に理解する上で重要なモデルですので説明をします。\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1312.6114.pdf\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://arxiv.org/pdf/1312.6114.pdf\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/38dce7c2a04f38d807194c1d6392fd38fe29b62c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f63353633326332622d316534302d303833382d656530342d3437316536366133346164662e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fc5632c2b-1e40-0838-ee04-471e66a34adf.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=ec227451df37789beb767c757553f9e4\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/c5632c2b-1e40-0838-ee04-471e66a34adf.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fc5632c2b-1e40-0838-ee04-471e66a34adf.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=bcbb71a754415cb426ffac69fed5ba87 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e標準正規分布からサンプリングした潜在変数 z から画像等データを生成することのできるモデルです。\u003cbr\u003e\n\u003ca href=\"https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24\" id=\"reference-554853f8b4d6cb157abf\"\u003eVariational Autoencoder徹底解説\u003c/a\u003e がとても詳しくわかりやすく書かれておりおすすめです。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"vae-ができること\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vae-%E3%81%8C%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVAE ができること\u003c/h3\u003e\n\n\u003cp\u003e標準正規分布 $\\mathscr{N}(0, I)$ から適当な潜在変数 z をサンプリングして VAE に入力することで、学習データをうまく補完したようなデータを生成できます。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/caa898648b2e5524af4b6ec69281a6c98ff3bb88/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f38313236326233362d383538382d663263642d663933642d3833636263356131366231642e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F81262b36-8588-f2cd-f93d-83cbc5a16b1d.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=29a47ad1b1d08169e89193e668d60146\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/81262b36-8588-f2cd-f93d-83cbc5a16b1d.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F81262b36-8588-f2cd-f93d-83cbc5a16b1d.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=89ce22dda75aecc2cb0d8c73f924097a 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://camo.qiitausercontent.com/d201e93374732ae33e37ae1a9088fe3beb11f928/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f62653866616232632d663165352d343837632d396232652d3633316537643761333364652e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fbe8fab2c-f1e5-487c-9b2e-631e7d7a33de.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=0ed7df958a47135202190d0f483cfb7b\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/be8fab2c-f1e5-487c-9b2e-631e7d7a33de.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fbe8fab2c-f1e5-487c-9b2e-631e7d7a33de.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=939ad53510fc60fb95390617c6358c69 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003cbr\u003e\nこの例では画像を生成していますが、言語の生成への応用なども研究されています。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"vae-の数学的な考え方\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vae-%E3%81%AE%E6%95%B0%E5%AD%A6%E7%9A%84%E3%81%AA%E8%80%83%E3%81%88%E6%96%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVAE の数学的な考え方\u003c/h3\u003e\n\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1312.6114.pdf\" rel=\"nofollow noopener\" target=\"_blank\"\u003eAuto-Encoding Variational Bayes\u003c/a\u003e で提案された、潜在変数\u003cbr\u003e\n z からデータ x が生成される場合の汎用な数学的モデルがまずあり、 VAE はその例として書かれているものです。\u003c/p\u003e\n\n\u003ch4\u003e\n\u003cspan id=\"auto-encoding-variational-bayes\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#auto-encoding-variational-bayes\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eAuto-Encoding Variational Bayes\u003c/h4\u003e\n\n\u003cp\u003eまずは VAE の元になっている数学的モデルの説明をします。\u003cbr\u003e\n潜在変数 z の事前分布 $P_\\theta(z)$ があり、 $P_\\theta(x|z)$ によって x が生成されるとします。\u003cbr\u003e\nこの時点では $P_\\theta(z)$, $P_\\theta(x|z)$ はその確率密度関数が $\\theta, z$ 両方に関してほぼ全体で微分可能という仮定はありますが、とくにそれらが正規分布だとかいう仮定はありません。\u003c/p\u003e\n\n\u003cp\u003eこのようなモデルでの対数尤度を最大化することを考えます。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eL=\\sum_xlogP_\\theta(x)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e目的は上の対数尤度 L を最大化する $\\theta$ を見つけることになります。\u003cbr\u003e\nL を最大化するには各 $logP_\\theta(x)$ を最大化すれば良いです。\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://qiita.com/halhorn/private/646d323ac457715866d4#%E4%BB%98%E9%8C%B21-vae-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA\"\u003e付録1、 Lower Bound の導出\u003c/a\u003eより、適当な分布 $Q_\\phi(z|x)$ に対して以下が言えます。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(x) \\geqq -KL[Q_\\phi(z|x)||P_\\theta(z)] + E_{Q_\\phi(z|x)}[logP_\\theta(x|z)]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cul\u003e\n\u003cli\u003e$Q_\\phi(z|x)$ は事後分布 $P_\\theta(z|x)$ の近似（付録1参照）\u003c/li\u003e\n\u003cli\u003e$KL[Q||P]$: \u003ca href=\"https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%AB%E3%83%90%E3%83%83%E3%82%AF%E3%83%BB%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%BC%E6%83%85%E5%A0%B1%E9%87%8F\" rel=\"nofollow noopener\" target=\"_blank\"\u003eKullback-Leibler divergence\u003c/a\u003e つまり Q, P ２つの分布の差異。非負。\u003c/li\u003e\n\u003cli\u003e$E_Q(P)$: 確率分布QでのPの期待値。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eこの右辺を Lower Bound と呼び、この右辺を最大化することによって $logP_\\theta$ を最大化します。\u003cbr\u003e\nつまり、 KL を小さくして E を大きくすれば良いわけです。\u003c/p\u003e\n\n\u003ch4\u003e\n\u003cspan id=\"vae-1\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vae-1\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVAE\u003c/h4\u003e\n\n\u003cp\u003eここで、　P, Q の条件付き確率に対して、条件を入力としてその確率分布を出力するニューラルネットワークを使うことを考えます。\u003cbr\u003e\nこれが VAE です。\u003cbr\u003e\nVAE では $P_\\theta(z)$ は標準正規分布 $\\mathscr{N}(0,I)$ を仮定します。\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e$Q_\\phi(z|x)$: \u003cstrong\u003eEncoder\u003c/strong\u003e\n\n\u003cul\u003e\n\u003cli\u003ex を入力として z の分布を出力\n\n\u003cul\u003e\n\u003cli\u003eVAE では正規分布を仮定\u003c/li\u003e\n\u003cli\u003e$\\mathscr{N}(\\mu(x), \\sigma(x))$ の $\\mu, \\sigma$ を出力するニューラルネット\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$logP_\\theta(x|z)$: \u003cstrong\u003eDecoder\u003c/strong\u003e\n\n\u003cul\u003e\n\u003cli\u003ez を入力として x の分布を出力\n\n\u003cul\u003e\n\u003cli\u003eVAE では正規分布もしくはベルヌーイ分布を仮定\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e右辺第一項の $KL[Q_\\phi(z|x)||P_\\theta(z)]$ は Encoder をできるだけ $P_\\theta(z)$ に近づければ小さくなります。\u003cbr\u003e\n$P_\\theta(z)$ は標準正規分布としたので、 $logP_\\theta(x)$ を大きくするには $Q_\\phi(z|x)$ (Encoder) を標準正規分布に近づくよう学習させればよいことになります。\u003c/p\u003e\n\n\u003cp\u003e右辺第二項の $E_{Q_\\phi(z|x)}[logP_\\theta(x|z)]$ は $x$ を Encoder への入力として $z$ を生成し、その $z$ を更に Decoder に入力して $x^\\prime$ を出力するニューラルネットとみなせます。ですのでこの出力 $x^\\prime$ が真の分布に近づくよう、大元の入力 $x$ との誤差を小さくするよう学習させることで $logP_\\theta(x)$ を大きくできます。\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://camo.qiitausercontent.com/b28aa4e7c970a1f8e355bbd0eddb0e1093ceb0f8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f62323431383136642d666663612d363135612d396232362d6235343966613663646335662e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fb241816d-ffca-615a-9b26-b549fa6cdc5f.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=41f595eeeda8ab2b884147e0f153d1ef\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/b241816d-ffca-615a-9b26-b549fa6cdc5f.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fb241816d-ffca-615a-9b26-b549fa6cdc5f.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=d6435c0ee033e6a59588bbd4d48e556c 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"vhred\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vhred\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVHRED\u003c/h1\u003e\n\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1605.06069.pdf\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://arxiv.org/pdf/1605.06069.pdf\u003c/a\u003e\u003cbr\u003e\n実装： \u003ca href=\"https://github.com/julianser/hed-dlg-truncated\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/julianser/hed-dlg-truncated\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/f41ed0745eb83b0278272a70f3c8d89370cd3abe/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f32663132633863332d323837662d313535332d383737612d3337366437613466336666302e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F2f12c8c3-287f-1553-877a-376d7a4f3ff0.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=cda12235bf193815c9508472b3cd01d9\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/2f12c8c3-287f-1553-877a-376d7a4f3ff0.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F2f12c8c3-287f-1553-877a-376d7a4f3ff0.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=e14ab6ebb7072510980b2baaf9e63003 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003cbr\u003e\n上のネットワーク図を見て分かる通り、 HRED に対して VAE の潜在変数 z を組み合わせたモデルです。\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"vhred-でできること\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vhred-%E3%81%A7%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVHRED でできること\u003c/h2\u003e\n\n\u003cp\u003eHRED と同じく過去の n-1 個の発話を与えられて、 n 個目の発話を生成します。\u003cbr\u003e\nただし、 HRED は対話学習において以下の問題を持っておりこれを解決することを目的にしています\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eHRED は確率的な多様性が字面にしかなく、会話の「流れ」のようなロングタームな多様性が無い。\n\n\u003cul\u003e\n\u003cli\u003e Encoder RNN, Context RNN, Decoder RNN のうち確率的な処理が Decoder RNN の次ステップの単語を生成する部分にしか無いから。\u003c/li\u003e\n\u003cli\u003e これによって、同じコンテキスト（発話リスト）を与えられても、答えの内容が毎回会話の流れとしては同じものしか出せない。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e HRED は短く情報量に乏しい答えをしがちである。\n\n\u003cul\u003e\n\u003cli\u003e 同じコンテキスト（発話リスト）を与えられても、それに続く発話は全く異なるものでありうる\n\n\u003cul\u003e\n\u003cli\u003e 「おはよう」「やあおはよう」 というコンテキストに対し 「いい天気だね」 も 「昨日の件どうなった？」も会話として成立する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e これらを決定論的に学習しようとすると、結果「無難な」答えつまり短いよくある答えを学ぶ傾向がある。\n\n\u003cul\u003e\n\u003cli\u003e 「うん」「そうだね」「・・・」など。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eこれに対し、 VHRED では Context RNN の部分に確率的なノイズを与えて学習することで上記の問題を解決します。\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eVHRED は会話の流れを表す Context RNN にノイズを乗せることで、同じコンテキストに対しても字面だけではない多様な返答ができる\u003c/li\u003e\n\u003cli\u003eVHRED はコンテキストに対する返答のばらつきを Context RNN の確率的な幅で吸収することでそれらをうまく学習できる\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e特に論文では VHRED では HRED などの従来の会話モデルに比べより長い文章を生成する傾向があることが書かれています。\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"vhredの数式的な理解\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vhred%E3%81%AE%E6%95%B0%E5%BC%8F%E7%9A%84%E3%81%AA%E7%90%86%E8%A7%A3\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVHREDの数式的な理解\u003c/h2\u003e\n\n\u003cp\u003eVHRED は　HRED に VAE の潜在変数の概念を追加したものとみなせますが、 HRED 側から入るよりも VAE 側から数式的に理解していくほうが近道です。論文の数式を VAE と比較しながら読み解いていきます。\u003c/p\u003e\n\n\u003cp\u003eVHRED は、 $w_i$ で表される、 i 番目の発話が i=1, ..., n-1 まで並んだ状態での、 $w_n$ の発話について考える問題となっています。\u003cbr\u003e\nここで n は現在の発話の数で、一つの会話全体で N 個の発話があるとします。\u003cbr\u003e\n各発話 $w_i$ は各単語等トークン $w_{i,1}, ..., w_{i,m}$ から成っているとします。\u003c/p\u003e\n\n\u003cp\u003e例えば、「おなかが減った」「そろそろ行く？」「ラーメンがいいな」という文章であれば\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e$w_1$: おなかが減った\n\n\u003cul\u003e\n\u003cli\u003e$w_{1,1}$: おなか\u003c/li\u003e\n\u003cli\u003e$w_{1,2}$: が\u003c/li\u003e\n\u003cli\u003e...\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$w_2$: そろそろ行く？\u003c/li\u003e\n\u003cli\u003e...\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eなどとなります。\u003c/p\u003e\n\n\u003cp\u003eVHRED では $logP_\\theta(w_1, ..., w_N)$ を観測された w のセットに対し最大化しようとします。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"潜在変数-z-の分布\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%BD%9C%E5%9C%A8%E5%A4%89%E6%95%B0-z-%E3%81%AE%E5%88%86%E5%B8%83\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e潜在変数 z の分布\u003c/h3\u003e\n\n\u003cp\u003eVAE では z は標準正規分布 $\\mathscr{N}(0, I)$ に従いますが、 VHRED では $z_n$ 以下の $\\mu_{prior}, \\sigma_{prior}$ という関数によって平均と分散が決まる正規分布に従うとされます。\u003cbr\u003e\n（付録1の VAE の式変形上は Q は必ずしも標準正規分布である必要は無いですね。）\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eP_\\theta(z_n|w_1, ..., w_{n-1}) = \\mathscr{N}(\\mu_{prior}(w_1, ..., w_{n-1}), \\sigma_{prior}(w_1, ..., w_{n-1}))\\\\\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e添字が n-1 までで n は含まれないのがキーポイントになってきます。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"尤度-logp-を最大化する\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%B0%A4%E5%BA%A6-logp-%E3%82%92%E6%9C%80%E5%A4%A7%E5%8C%96%E3%81%99%E3%82%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e尤度 logP を最大化する\u003c/h3\u003e\n\n\u003cp\u003eVAE と同じく Lower Bound が求められそれを最大化します。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(w_1, ..., w_N) \\geqq \\sum_{n=1}^N\\left\\{ -KL[Q_\\phi(z_n|w_1, ..., w_n)||P_\\theta(z_n|w_1, ..., w_{n-1})] + E_{Q_\\phi(z_n|w_1, ..., w_n)}[logP_\\theta(w_n|z_n, w_1, ..., w_{n-1})] \\right\\}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eこの式を注意深く見てみると、添字が $n$ の部分と $n-1$ の部分が入り混じっています。実はこの数式の $w_{n-1}$ と $w_n$ の間にはとても大きな溝があります。\u003cbr\u003e\nVAE での $logP_\\theta$ は\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(x) \\geqq -KL[Q_\\phi(z|x)||P_\\theta(z)] + E_{Q_\\phi(z|x)}[logP_\\theta(x|z)]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eでしたが、ここで\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e$z$ -\u0026gt; $z_n$\u003c/li\u003e\n\u003cli\u003e$x$ -\u0026gt; $w_n$\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eとして、各確率に条件 $|w_1, ..., w_{n-1}$ をつけてnを1~Nまで和をとると VHRED の式になることがわかります。（\u003ca href=\"https://qiita.com/halhorn/private/646d323ac457715866d4#%E4%BB%98%E9%8C%B22-vhred-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA\"\u003e付録2\u003c/a\u003e）\u003cbr\u003e\nVHRED の式で $w_n$ を $x$ としてみるとわかりやすいかもしれません。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(w_1, ..., w_N) \\geqq \\sum_{n=1}^N\\left\\{ -KL[Q_\\phi(z_n|x, w_1, ...,w_{n-1})||P_\\theta(z_n|w_1, ..., w_{n-1})] + E_{Q_\\phi(z_n|x, w_1, ..., w_{n-1})}[logP_\\theta(x|z_n, w_1, ..., w_{n-1})] \\right\\}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eつまり VHRED は数式的には、\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e$w_1, ..., w_{n-1}$ が事前に与えられている状態での\u003c/li\u003e\n\u003cli\u003e$w_n$ と $z_n$ での VAE\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eと見ることができます。\u003cbr\u003e\nVAE では $Q_\\phi(z|x)$ は $\\mathscr{N}(\\mu(x), \\sigma(x))$ であるとされましたが、 VHRED では $\\mathscr{N}(\\mu_{posterior}(w_1, ..., w_n), \\sigma_{posterior}(w_1, ..., w_n))$ とします。\u003cbr\u003e\n（上に出てきた \u003ccode\u003eprior\u003c/code\u003e の方は $w_1, ..., w_{n-1}$ のみなのに対して、この \u003ccode\u003eposterior\u003c/code\u003e は $w_n$ も入っていますね。）\u003c/p\u003e\n\n\u003cp\u003eVAE で $Q_\\phi(z|x)$ の $\\mu, \\sigma$ を $P_\\theta(z) = \\mathscr{N}(0,I)$ つまり0, I に近づけたように、 VHRED では $\\mu_{posterior}, \\sigma_{posterior}$ を $\\mu_{prior}, \\sigma_{prior}$ に近づけるよう学習を行います。\u003c/p\u003e\n\n\u003cp\u003eこれを VAE 風の図で書いてみるとこのようになります。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/7f95b43f79b81b6b6d07209728e7a829f551597b/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f37656133363064652d643666392d663433382d633938332d3234623336323136653264632e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F7ea360de-d6f9-f438-c983-24b36216e2dc.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=0bec207c9b42f1b6844ccef6cf205c87\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/7ea360de-d6f9-f438-c983-24b36216e2dc.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F7ea360de-d6f9-f438-c983-24b36216e2dc.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=d59da54338811a135914080d4ea9cff1 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"vhred-がやろうとしていること数式的な方向から\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vhred-%E3%81%8C%E3%82%84%E3%82%8D%E3%81%86%E3%81%A8%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E3%81%93%E3%81%A8%E6%95%B0%E5%BC%8F%E7%9A%84%E3%81%AA%E6%96%B9%E5%90%91%E3%81%8B%E3%82%89\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVHRED がやろうとしていること：数式的な方向から\u003c/h3\u003e\n\n\u003cp\u003e生成も学習も、 $w_1, ..., w_{n-1}$ によって条件付けされた状態での潜在変数 $z_n$ の学習と、その $z_n$ からの $w_n$ つまり発話生成を行っていると考えられます。\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e学習\n\n\u003cul\u003e\n\u003cli\u003e$w_1, ..., w_{n-1}$ によって条件付けされた状態で\u003c/li\u003e\n\u003cli\u003e$w_n$ を入力として（潜在変数 $z_n$ を介して） $w_n$ を出力する学習\u003c/li\u003e\n\u003cli\u003e$z_n$ も $w_1, ..., w_{n-1}$ によって条件付けされたある正規分布に従うよう正則化がされる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e生成\n\n\u003cul\u003e\n\u003cli\u003e$w_1, ..., w_{n-1}$ によって条件付けされた状態で\u003c/li\u003e\n\u003cli\u003e$z_n$ をサンプリングし\u003c/li\u003e\n\u003cli\u003e（VAE の文脈での）Decoder で $w_n$ を生成する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"vhred-のニューラルネット的な理解\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vhred-%E3%81%AE%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E7%9A%84%E3%81%AA%E7%90%86%E8%A7%A3\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVHRED のニューラルネット的な理解\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://camo.qiitausercontent.com/f41ed0745eb83b0278272a70f3c8d89370cd3abe/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f32663132633863332d323837662d313535332d383737612d3337366437613466336666302e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F2f12c8c3-287f-1553-877a-376d7a4f3ff0.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=cda12235bf193815c9508472b3cd01d9\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/2f12c8c3-287f-1553-877a-376d7a4f3ff0.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2F2f12c8c3-287f-1553-877a-376d7a4f3ff0.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=e14ab6ebb7072510980b2baaf9e63003 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003cbr\u003e\n再び VHRED のモデル図です。\u003cbr\u003e\n先程の数式的な方面からの理解により、なぜ学習時に \u003ccode\u003eposterior parametarization\u003c/code\u003e つまり今生成しようとしている次の発話 $w_n$ が入力になってるんだ？とかが理解できると思います。\u003c/p\u003e\n\n\u003cp\u003e先程の数式とネットワークが以下のように対応します\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e$|w_1, ..., w_{n-1}$ の条件： n-1 までのデータを Encoder RNN と Context RNN に入力したときの Context RNN の final state\u003c/li\u003e\n\u003cli\u003e$w_n$:\n\n\u003cul\u003e\n\u003cli\u003eエンコード時： n のデータでの Encoder RNN\u003c/li\u003e\n\u003cli\u003eデコード時： Decoder RNN の出力\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$Q_\\phi(z_n|w_n, w_1, ..., w_{n-1})$: Context RNN と Encoder RNN の final state を FNN に食わせたもの\n\n\u003cul\u003e\n\u003cli\u003e出力は $\\mu_{posterior}, \\sigma_{posterior}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$P_\\theta(w_n|z_n, w_1, ..., w_{n-1})$: Decoder RNN\n\n\u003cul\u003e\n\u003cli\u003e入力\n\n\u003cul\u003e\n\u003cli\u003e$\\mu_{posterior}, \\sigma_{posterior}$ からサンプリングされた z\u003c/li\u003e\n\u003cli\u003eContext RNN の final state\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"github-のコードでのネットワーク\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#github-%E3%81%AE%E3%82%B3%E3%83%BC%E3%83%89%E3%81%A7%E3%81%AE%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eGithub のコードでのネットワーク\u003c/h3\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/julianser/hed-dlg-truncated/blob/master/dialog_encdec.py#L1723\" rel=\"nofollow noopener\" target=\"_blank\"\u003e本家の Github に上がっているコード\u003c/a\u003eをざっと見た感じ以下のようなネットワークになっているようです。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/6c701c2792ddd4ce3a10081e40644754346b46c9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f65613862333866362d386665622d373663392d333531392d6334636364376134316239662e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fea8b38f6-8feb-76c9-3519-c4ccd7a41b9f.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=22ea71a360d85c7291e8c5391fb43d33\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/61079/ea8b38f6-8feb-76c9-3519-c4ccd7a41b9f.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fea8b38f6-8feb-76c9-3519-c4ccd7a41b9f.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=b20b0582fbbabcf77dff9dc3a1288ad4 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eボックス： 処理とその返り値の変数名\n\n\u003cul\u003e\n\u003cli\u003e一行目： 返り値の変数名\u003c/li\u003e\n\u003cli\u003e二行目： その処理をおこなう関数など\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e太い茶色枠は HRED にもある要素\u003c/li\u003e\n\u003cli\u003eプログラムは設定によって様々な構成の NN を作れるよう書かれているので、図では図右上の条件でのものを書いています\u003c/li\u003e\n\u003cli\u003e一部省略した部分などもあります\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"おわりに\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eおわりに\u003c/h1\u003e\n\n\u003cp\u003e最初は VHRED 論文の自身での整理とチームメンバーへの共有の目的で書き始めたこの記事ですが、途中から Advent Calendar に出そうという気持ちになり、 Deep Learning の対話モデルの話にしようと風呂敷を広げ続けた結果、このような長大な文章になってしまったことをお詫びいたします。\u003cbr\u003e\nDeepLearning といえば画像！ということで画像・CNN から Deep Learning に入った方は多いと思いますが、自然言語処理、とくに言語の生成に興味を持ってくれる Deep Learner が増えたらいいなと思っています。\u003c/p\u003e\n\n\u003cp\u003e私自身大学時代にロボットの視覚と運動指令を結びつけるモデルとして RNN は使っていたものの、今のチームに移動してから初めて自然言語処理で Deep Learning をはじめた人です。\u003cbr\u003e\nロボットを動かすのにせよ、画像を生成するのにせよ、自然言語を生成するのにせよ、何かを生成できるモデルというのはとてもおもしろいです。何か生き物のようなものを感じます。\u003cbr\u003e\nこの面白さに触れられる人が増えますように。\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"付録\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E4%BB%98%E9%8C%B2\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e付録\u003c/h1\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"付録0-よく使う式変形定義\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E4%BB%98%E9%8C%B20-%E3%82%88%E3%81%8F%E4%BD%BF%E3%81%86%E5%BC%8F%E5%A4%89%E5%BD%A2%E5%AE%9A%E7%BE%A9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e付録0. よく使う式変形・定義\u003c/h2\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"a-確率の積分\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#a-%E7%A2%BA%E7%8E%87%E3%81%AE%E7%A9%8D%E5%88%86\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003ea. 確率の積分\u003c/h3\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\\int_zP(z)dz = 1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"b-期待値\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#b-%E6%9C%9F%E5%BE%85%E5%80%A4\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eb. 期待値\u003c/h3\u003e\n\n\u003cp\u003e確率密度関数 $Q(z)$ に対し値 $X(z)$ の期待値\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eE_{Q(z)}[X(z)] = \\int_zX(z)Q(z)dz\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"c-ベイズの定理\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#c-%E3%83%99%E3%82%A4%E3%82%BA%E3%81%AE%E5%AE%9A%E7%90%86\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003ec. ベイズの定理\u003c/h3\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eP(x,z) = P(z|x)P(x)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"d-kullback-leibler-divergence\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#d-kullback-leibler-divergence\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003ed. Kullback-Leibler divergence\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%AB%E3%83%90%E3%83%83%E3%82%AF%E3%83%BB%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%BC%E6%83%85%E5%A0%B1%E9%87%8F\" rel=\"nofollow noopener\" target=\"_blank\"\u003eカルバックライブラー情報量\u003c/a\u003e。２つの分布 P, Q の差。非負。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eKL[P(z)||Q(z)] = \\int_z P(z)log\\left\\{\\frac{P(z)}{Q(z)}\\right\\}dz\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"付録1-vae-lower-bound-の導出\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E4%BB%98%E9%8C%B21-vae-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e付録1. VAE Lower Bound の導出\u003c/h2\u003e\n\n\u003cp\u003e$logP_\\theta(x)$ に対し任意の分布 Q を組み込んで KL の式が出るように変形していくことで下限（Lower Bound）を求めます。\u003cbr\u003e\nなぜそんな式変形を・・となりますがパターンです。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(x) = \\int_zQ_\\phi(z|x)logP(x)dz\n ~~ \\verb|←a. より追加された部分は1|\\\\\n\n= \\int_zQ_\\phi(z|x)log\\left\\{\\frac{P_\\theta(x,z)}{P_\\theta(z|x)}\\right\\}dz\n ~~ \\verb|←c. ベイズの定理|\\\\\n\n= \\int_zQ_\\phi(z|x)log\\left\\{\\frac{Q_\\phi(z|x)}{P_\\theta(z|x)}\\frac{P_\\theta(x,z)}{Q_\\phi(z|x)}\\right\\}dz\n~~ \\verb|←分子分母にQをかけた|\\\\\n\n= \\int_zQ_\\phi(z|x)log\\left\\{\\frac{Q_\\phi(z|x)}{P_\\theta(z|x)}\\right\\}dz\n+ \\int_zQ_\\phi(z|x)log\\left\\{\\frac{P_\\theta(x,z)}{Q_\\phi(z|x)}\\right\\}dz\\\\\n\n= KL[Q_\\phi(z|x)||P_\\theta(z|x)] + \\int_zQ_\\phi(z|x)log\\left\\{\\frac{P_\\theta(x,z)}{Q_\\phi(z|x)}\\right\\}dz\n~~ \\verb|↑d. 左辺は KL|\\\\\n\n\\geqq \\int_zQ_\\phi(z|x)log\\left\\{\\frac{P_\\theta(x,z)}{Q_\\phi(z|x)}\\right\\}dz\n~~ \\verb|← KL は非負（≧0）|\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eまとめると、この右辺を $L_b$ として以下の不等式が成り立ちます。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(x) \\geqq L_b \\\\\n\\left(L_b = \\int_zQ_\\phi(z|x)log\\left\\{\\frac{P_\\theta(x,z)}{Q_\\phi(z|x)}\\right\\}dz \\right)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eちなみに上に出てきた $KL[Q_\\phi(z|x)||P_\\theta(z|x)]$ は、事後確率 $Q_\\phi(z|x), P_\\theta(z|x)$ が近いほど0に近づきます。\u003cbr\u003e\n$L_b$ を大きくしようとすると上記の KL は小さくなるので、 Q は P に近づいていきます。すなわち $Q_\\phi(z|x)$ は $P_\\theta(z|x)$ の近似とみなせます。\u003c/p\u003e\n\n\u003cp\u003e次にこの $L_b$ が KL と期待値となることを示します。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eL_b = \\int_zQ_\\phi(z|x)log\\left\\{\\frac{P_\\theta(x,z)}{Q_\\phi(z|x)}\\right\\}dz \\\\\n= \\int_zQ_\\phi(z|x)log\\left\\{\\frac{P_\\theta(z)}{Q_\\phi(z|x)}P_\\theta(x|z)\\right\\}dz\n~~ \\verb|← c. ベイズの定理|\\\\\n\n= - \\int_zQ_\\phi(z|x)log\\left\\{\\frac{Q_\\phi(z|x)}{P_\\theta(z)}\\right\\}dz\n+ \\int_zQ_\\phi(z|x)logP_\\theta(x|z)dz\\\\\n\n= -KL[Q_\\phi(z|x)||P_\\theta(z)] + E_{Q_\\phi(z|x)}[logP_\\theta(x|z)]\n~~ \\verb|←d. KL と b. 期待値|\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eこれらをまとめて、\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(x) \\geqq -KL[Q_\\phi(z|x)||P_\\theta(z)] + E_{Q_\\phi(z|x)}[logP_\\theta(x|z)]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"付録2-vhred-lower-bound-の導出\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E4%BB%98%E9%8C%B22-vhred-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e付録2. VHRED Lower Bound の導出\u003c/h2\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(w_1, ..., w_N) = \\sum_{n=1}^NlogP_\\theta(w_n|w_1, ..., w_{n-1})\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eあとは\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"math\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003elogP_\\theta(w_n|w_1, ..., w_{n-1})\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eに対して、 VAE の式を\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e$z$ -\u0026gt; $z_n$\u003c/li\u003e\n\u003cli\u003e$x$ -\u0026gt; $w_n$\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eとして、各確率に条件 $|w_1, ..., w_{n-1}$ をつけて全く同じ計算をするだけです。\u003c/p\u003e\n","createdAt":"2017-11-28T09:21:06Z","elapsedYearsFromLastModifiedAt":6,"encryptedId":"BAhJIhNBcnRpY2xlLTU0NTA1MgY6BkVU--84ed98dd6666e74990a23134e65b71c102c044eedc6885814f26dbc27d520772","isBanned":false,"isDeprecated":true,"isDestroyableByViewer":false,"isEditRequestReadableByViewer":true,"isEditRequestSendableByViewer":true,"isLikableByViewer":true,"isLikedByViewer":false,"isPublic":true,"isSlide":false,"isStockableByViewer":true,"isStockedByViewer":false,"isSubscribableByViewer":true,"isSubscribedByViewer":false,"isUpdatableByViewer":false,"isUpdated":true,"lastModifiedAt":"2017-12-02T22:01:33Z","publishedAt":"2017-12-02T22:01:33Z","likesCount":156,"linkUrl":"https://qiita.com/halhorn/items/646d323ac457715866d4","organization":{"encryptedId":"BAhJIhRPcmdhbml6YXRpb24tMjcGOgZFVA==--4a59c3734baef5161d3e7b658ec0dfabccb7f05a0a4d067e13b5c96a8c95247e","isFollowedByViewer":false,"name":"株式会社MIXI","urlName":"mixi","logoUrl":"https://s3-ap-northeast-1.amazonaws.com/qiita-organization-image/8dcc06a2c30582bb59e4e5a4fedd64c5d7bf0f85/original.jpg?1649176405"},"stockedCount":136,"title":"DeepLearning における会話モデル： Seq2Seq から VHRED まで","toc":"\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E6%A6%82%E8%A6%81\"\u003e概要\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E4%BC%9A%E8%A9%B1%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%82%82%E3%82%8D%E3%82%82%E3%82%8D\"\u003e会話モデルのもろもろ\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#seq2seq\"\u003eSeq2Seq\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E6%A7%8B%E9%80%A0\"\u003e構造\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%82%82%E3%81%86%E3%81%99%E3%81%93%E3%81%97%E5%85%B7%E4%BD%93%E7%9A%84%E3%81%AB\"\u003eもうすこし具体的に\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8\"\u003eできること\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#hred\"\u003eHRED\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E6%A7%8B%E9%80%A0-1\"\u003e構造\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#vae\"\u003eVAE\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#vae-%E3%81%8C%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8\"\u003eVAE ができること\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#vae-%E3%81%AE%E6%95%B0%E5%AD%A6%E7%9A%84%E3%81%AA%E8%80%83%E3%81%88%E6%96%B9\"\u003eVAE の数学的な考え方\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#auto-encoding-variational-bayes\"\u003eAuto-Encoding Variational Bayes\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#vae-1\"\u003eVAE\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#vhred\"\u003eVHRED\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#vhred-%E3%81%A7%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8\"\u003eVHRED でできること\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#vhred%E3%81%AE%E6%95%B0%E5%BC%8F%E7%9A%84%E3%81%AA%E7%90%86%E8%A7%A3\"\u003eVHREDの数式的な理解\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E6%BD%9C%E5%9C%A8%E5%A4%89%E6%95%B0-z-%E3%81%AE%E5%88%86%E5%B8%83\"\u003e潜在変数 z の分布\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E5%B0%A4%E5%BA%A6-logp-%E3%82%92%E6%9C%80%E5%A4%A7%E5%8C%96%E3%81%99%E3%82%8B\"\u003e尤度 logP を最大化する\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#vhred-%E3%81%8C%E3%82%84%E3%82%8D%E3%81%86%E3%81%A8%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E3%81%93%E3%81%A8%E6%95%B0%E5%BC%8F%E7%9A%84%E3%81%AA%E6%96%B9%E5%90%91%E3%81%8B%E3%82%89\"\u003eVHRED がやろうとしていること：数式的な方向から\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#vhred-%E3%81%AE%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E7%9A%84%E3%81%AA%E7%90%86%E8%A7%A3\"\u003eVHRED のニューラルネット的な理解\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#github-%E3%81%AE%E3%82%B3%E3%83%BC%E3%83%89%E3%81%A7%E3%81%AE%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF\"\u003eGithub のコードでのネットワーク\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB\"\u003eおわりに\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E4%BB%98%E9%8C%B2\"\u003e付録\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E4%BB%98%E9%8C%B20-%E3%82%88%E3%81%8F%E4%BD%BF%E3%81%86%E5%BC%8F%E5%A4%89%E5%BD%A2%E5%AE%9A%E7%BE%A9\"\u003e付録0. よく使う式変形・定義\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#a-%E7%A2%BA%E7%8E%87%E3%81%AE%E7%A9%8D%E5%88%86\"\u003ea. 確率の積分\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#b-%E6%9C%9F%E5%BE%85%E5%80%A4\"\u003eb. 期待値\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#c-%E3%83%99%E3%82%A4%E3%82%BA%E3%81%AE%E5%AE%9A%E7%90%86\"\u003ec. ベイズの定理\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#d-kullback-leibler-divergence\"\u003ed. Kullback-Leibler divergence\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E4%BB%98%E9%8C%B21-vae-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA\"\u003e付録1. VAE Lower Bound の導出\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E4%BB%98%E9%8C%B22-vhred-lower-bound-%E3%81%AE%E5%B0%8E%E5%87%BA\"\u003e付録2. VHRED Lower Bound の導出\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n","totalPv":29235,"uuid":"646d323ac457715866d4","banReason":null,"adventCalendarItem":{"day":3,"calendar":{"name":"mixiグループ","urlName":"mixi","year":2017,"organization":null}},"author":{"encryptedId":"BAhJIg9Vc2VyLTYxMDc5BjoGRVQ=--5608acdc114ea5cdec23616b13a040b0ffea23fea9c15e69a1da572d1371aa9b","isBlockingViewer":false,"isFollowableByViewer":true,"isFollowedByViewer":false,"isTweetWebNotificationReceivable":true,"profileImageUrlW48":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F61079%2Fprofile-images%2F1473695426?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=1705e4b22169c3dd535fe8e150abf298","urlName":"halhorn","twitterUrlName":"halhorn","name":"Harumitsu Nobuta","organizations":{"edges":[{"node":{"urlName":"mixi"}}]}},"tags":[{"name":"NLP","urlName":"nlp"},{"name":"DeepLearning","urlName":"deeplearning"},{"name":"VHRED","urlName":"vhred"}],"followingLikers":[],"recentlyFollowingLikers":[],"postingCampaign":null},"comments":[],"client":null,"adsEventEmitter":null}}</script>