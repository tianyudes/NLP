Attention Mechanism

1. Seq2Seqについて

    1.1 Seq2Seqの定義
    Seq2Seqモデルは一連の入力を別の一連の出力に変換するために設計された深層学習のフレームワークです。基本的な構造は、エンコーダ部とデコーダ部の2つの主要なコンポーネントから構成されています。

    1.2 seq2seqの構造
        
        1.2.1 encoder

        エンコーダは、入力シーケンス（例えば、テキストの文章や単語の列）を受け取り、それを固定長のベクトル表現に変換します。このベクトルは、入力シーケンスの「意味」や「文脈」を捉えたもので、デコーダが出力シーケンスを生成する際のコンテキスト情報として機能します。エンコーダは通常、RNN、LSTM、GRUなどの再帰的ニューラルネットワークで構成されています。

    

        1.2.2 decoder

        デコーダは、エンコーダから受け取った固定長のベクトルをもとに、ターゲットシーケンス（例えば、翻訳後の文章や要約文）を一つずつ生成していきます。デコーダもエンコーダと同様にRNN、LSTM、GRUなどで構成されることが多く、各ステップでの出力が次のステップの入力にも用いられます。デコーダは、エンコーダからの情報とこれまでに生成した出力を基に、次の単語（またはトークン）を予測します。
    
    ![seq2seq](../../image/Deep_learning/attention/1-seq2seq.gif)
    ![translation](../../image/Deep_learning/attention/1-2-translation.gif)


    1.3 seq2seqのディテール

    緑色のエンコーダーは、入力シーケンスを処理し、入力情報を取得します。これらの情報は黄色いベクトル（コンテキストベクトルと呼ばれます）に変換されます。入力シーケンス全体を処理し終えた後、エンコーダーはコンテキストベクトルを紫色のデコーダーに送信します。デコーダーは、コンテキストベクトル内の情報を通じて、新しいシーケンスを一つずつ出力します。

    ![seq2seq2](../../image/Deep_learning/attention/1-3-encoder-decoder.gif)
    ![seq2seq3](../../image/Deep_learning/attention/1-3-mt.gif)

    





   


    ![seq2seq](../../image/Deep_learning/seq2seq.png)
    ![seq2seq_example](../../image/Deep_learning/seq2seq_example.png)






    1.5 Seq2Seqの欠点

        Seq2Seqモデルにはいくつかの欠点があります。
        
        まず、入力シーケンスXの長さを無視しています。入力文が非常に長く、特に訓練セットの初期の文よりも長い場合、モデルの性能が急激に低下します。
        
        また、入力シーケンスXに対して区別をつけていません。入力Xを固定長でエンコードすると、文中の各単語に同じ重みを与えることになり、これはモデルの性能低下につながります。
        
        さらに、デコーダで最初の出力にエラーが発生すると、そのエラーが続くと修正されずに進行してしまいます。

    1.6 source code example



    1.7 その他

    https://qiita.com/soldier-tn/items/28f2f03c3058d671de12
    

2. Attention






